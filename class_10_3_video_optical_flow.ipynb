{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 👉 class_10_3 » _Optical Flow_ </center>\n",
    "\n",
    "## Optical Flow (광학 흐름)  \n",
    "\n",
    "Optical flow refers to __the movement pattern of an object__ in an image.   \n",
    "The distribution of the direction and distance traveled by the pixels between the previous and next frames.   \n",
    "With optical flow, __you can see how much and in which direction an object has moved in the image.__   \n",
    "In addition, additional calculations can be performed to__ predict the behavior of objects__.  \n",
    "\n",
    "        광학 흐름이란 영상 내 물체의 움직임 패턴을 말합니다.   \n",
    "        이전 프레임과 다음 프레임 간 픽셀이 이동한 방향과 거리 분포입니다.   \n",
    "        광학 흐름으로 영상 내 물체가 어느 방향으로 얼마만큼 움직였는지 파악할 수 있습니다.   \n",
    "        더불어 추가 연산을 하면 물체의 움직임을 예측할 수도 있습니다.\n",
    "\n",
    "<img src='./images/practice_img/op_flow.jpeg' width=500 height=200>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Optical_flow\n",
    "\n",
    "Optical flow assumes two facts:  \n",
    "1. The pixel intensity of moving objects between consecutive frames remains unchanged.    \n",
    "2. Neighboring pixels have similar movements.  \n",
    "\n",
    "        광학 흐름은 다음 두 가지 사실을 가정합니다.\n",
    "        1. 연속된 프레임 사이에서 움직이는 물체의 픽셀 강도(intensity)는 변함이 없다.  \n",
    "        2. 이웃하는 픽셀은 비슷한 움직임을 갖는다.  \n",
    "        \n",
    "There are two ways to calculate optical flow.   \n",
    "- __Sparse optical flow__: only some pixels are counted, __Lucas-Kanade Algorithm__ \n",
    "- __Dense optical flow__: Counts all pixels in the image, __Gunner Farneback Algorithm__  \n",
    "    \n",
    "        광학 흐름을 계산하는 방법은 두 가지입니다.   \n",
    "        - 희소(sparse) 광학 흐름 : 일부 픽셀만 계산, Lucas-Kanade Algorithm  \n",
    "        - 밀집(dense) 광학 흐름 : 영상 전체 픽셀을 모두 계산, Gunner Farneback Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ▶ Lucas-Kanade Algorithm \n",
    "\n",
    "The Lucas-Kanade algorithm is an algorithm that uses __the assumption that neighboring pixels behave similarly.__  \n",
    "\n",
    "- The Lucas-Kanade algorithm uses a small window (3 x 3 patches) to calculate movement.  \n",
    "- Due to the small size of the window, large object movement will cause problems.  \n",
    ">- Use an image pyramid to improve this problem. \n",
    ">- The farther up the image pyramid (the smaller the image), the less visible the small movement and the smaller the small movement.   \n",
    ">- It can detect even large movements like this.\n",
    "\n",
    "        루카스-카나데 알고리즘은 이웃하는 픽셀이 비슷하게 움직인다는 가정을 이용하는 알고리즘입니다.   \n",
    "        - 루카스-카나데 알고리즘은 작은 윈도(3 x 3 patch)를 사용하여 움직임을 계산합니다.  \n",
    "        - 윈도 크기가 작기 때문에 물체 움직임이 크면 문제가 생깁니다.  \n",
    "        - 이 문제를 개선하기 위해 이미지 피라미드를 사용합니다. \n",
    "        - 이미지 피라미드 위쪽으로 갈수록(이미지가 작아질수록) 작은 움직임은 티가 안 나고 큰 움직임은 작은 움직임 같아 보입니다.   \n",
    "          이렇게 큰 움직임도 감지할 수 있습니다. \n",
    "\n",
    "OpenCV **cv2.calcOpticalFlowPyrLK()** \n",
    "\n",
    "**nextPts, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, nextPts, status, err, wirnSize, maxLevel, criteria, flags, minEigThreshold)**  \n",
    "\n",
    ">- prevPts: prev Img cnr keypoints, use cv2.goodFeaturesToTrack()\n",
    ">- nextPst: next Img cnr keypoints,\n",
    ">- status:  Same length as nextPts, 1 if there is a correspondence point, 0 if it does not nextPts와 같은 길이/대응점이 있으면 1, 없으면 0\n",
    ">- err: correspondence point error\n",
    ">- winSize=(21,21): \n",
    ">- maxLevel=3: pyramid level\n",
    ">- criteria=(COUNT+EPS, 30, 0.01): Requirements to stop iterative exploration 반복 탐색 중지 요건  \n",
    ">>- cv2.TERM_CRITERIA_EPS: Stop if accuracy is less than epsilon 정확도가 epsilon보다 작으면 중지,   \n",
    ">>- cv2.TERM_CRITERIA_MAX_ITER: Stop when max_iter number of times is met max_iter 횟수를 채우면 중지,   \n",
    ">>- cv2.TERM_CRITERIA_COUNT: same as MAX_ITER\n",
    ">- flgs=0:   \n",
    ">>- 0: Using prevPts as the initial value of nextPts  prevPts를 nextPts의 초기 값으로 사용,   \n",
    ">>- cv2.OPTFLOW_USE_INITAL_FLOW: Using prevPts as the initial value of nextPts nextPts의 값을 초기 값으로 사용,   \n",
    ">>- cv2.OPTFLOW_LK_GET_MIN_EIGENVALS: Calculate error as the least unique value 오차를 최소 고유 값으로 계산  \n",
    ">- minEigThreshold=1e-4: Minimum threshold eigenvalue to use in calculating the correspondence point 대응점 계산에 사용할 최소 임계 고유 값\n",
    "\n",
    "    \n",
    "The above function does not count all the pixels in the video at once. \n",
    "- Calculate using only feature points obtained from the cv2.goodFeaturesToTrack() function.   \n",
    "- The prevImg and nextImg parameters can be used to pass the previous and next frames.   \n",
    "- The prevPts parameter passes the features from the previous frame.   \n",
    "- This calculates where the feature point moved in the next frame and returns it to nextPts.   \n",
    "- If the two features correspond to each other, the status variable will be 1, otherwise it will be 0.   \n",
    "- Also, if maxLevel=0 does not use image pyramids. \n",
    "\n",
    "        위 함수는 영상 내 픽셀 전체를 한번에 계산하지 않습니다. \n",
    "        - cv2.goodFeaturesToTrack() 함수로 얻은 특징점만 활용하여 계산합니다.   \n",
    "        - prevImg와 nextImg 파라미ㄷ터에는 이전 프레임과 다음 프레임을 전달하면 됩니다.   \n",
    "        - prevPts 파라미터에는 이전 프레임에서 얻은 특징점을 전달합니다.   \n",
    "        - 그러면 특징점이 다음 프레임에서 어디로 이동했는지 계산하여 nextPts로 반환합니다.   \n",
    "        - 두 특징점이 서로 대응하면 status 변수가 1, 그렇지 않으면 0이 됩니다.   \n",
    "        - 또한, maxLevel=0이면 이미지 피라미드를 사용하지 않습니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lucas-Kanade Algorithm calcOpticalFlowPyrLK (track_opticalLK.py)\n",
    "\n",
    "import numpy as np, cv2\n",
    "\n",
    "cap = cv2.VideoCapture('./Videos/walking2.mp4')\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "delay = int(500/fps)\n",
    "\n",
    "color = np.random.randint(0,255,(200,3)) # random color\n",
    "lines = None    \n",
    "prevImg = None  \n",
    "\n",
    "termcriteria =  (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03) # calcOpticalFlowPyrLK stop criteria\n",
    "# Parameters for Lucas-Kande optical flow\n",
    "# lk_params = dict(winSize = (15,15), maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10,1))\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    if prevImg is None:\n",
    "        prevImg = gray\n",
    "        lines = np.zeros_like(frame) \n",
    "        prevPt = cv2.goodFeaturesToTrack(prevImg, 200, 0.01, 10) # starting cnr detect\n",
    "    else:\n",
    "        nextImg = gray\n",
    "        nextPt, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, \\\n",
    "                                        prevPt, None, criteria=termcriteria)  # next cnr detect\n",
    "        prevMv = prevPt[status==1]\n",
    "        nextMv = nextPt[status==1]\n",
    "        for i,(p, n) in enumerate(zip(prevMv, nextMv)):\n",
    "            px,py = p.ravel()\n",
    "            nx,ny = n.ravel()\n",
    "            cv2.line(lines, (int(px), int(py)), (int(nx), int(ny)), color[i].tolist(), 2) \n",
    "            cv2.circle(img_draw, (int(nx), int(ny)), 2, color[i].tolist(), -1) \n",
    "        img_draw = cv2.add(img_draw, lines) \n",
    "        prevImg = nextImg \n",
    "        prevPt = nextMv.reshape(-1,1,2)\n",
    "\n",
    "    cv2.imshow('OpticalFlow-LK', img_draw)\n",
    "    key = cv2.waitKey(delay)\n",
    "    if key == 27 : \n",
    "        break\n",
    "    elif key == 8:       # Backspace:remove tracking history\n",
    "        prevImg = None\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cv2.goodFeatureToTrack() :  Detects the features of the previous frame   \n",
    "- cv2.calcOpticalFlowPyrLK() : Detects the features of the next frame   \n",
    "- Select only the best matching features from the previous frame and the next frame, and display them as lines and dots.     \n",
    "- Composite a tracking line into the original image to display the tracking line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ▶ Gunner Farneback Algorithm\n",
    "\n",
    "The Gunner Farneback Algorithm is __a dense algorithm that calculates optical flow using every pixel in an image.__   \n",
    "- Dense optical flow, unlike sparse optical flow, is calculated using the entire pixel of the image.   \n",
    "- So you don't have to pass on features to track.   \n",
    "- However, it is slow because it is calculated using all pixels.  \n",
    "\n",
    "        - Gunner Farneback Algorithm은 영상 전체의 픽셀을 활용해 광학 흐름을 계산하는 밀집 방식 알고리즘입니다.   \n",
    "        - 밀집 광학 흐름은 희소 광학 흐름과 다르게 영상 전체 픽셀을 활용해 계산합니다.   \n",
    "        - 그래서 추적할 특징점을 따로 전달할 필요가 없습니다. 다만, 전체 픽셀을 활용해 계산하므로 속도가 느립니다.  \n",
    "\n",
    "OpenCV \n",
    "\n",
    "- **flow = cv2.calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)**\n",
    ">- prev, next: prev, next frame\n",
    ">- flow: Optical flow calculations show that each pixel travels (the same size as the input) 광학 흐름 계산 결과, 각 픽셀이 이동한 거리 (입력과 동일한 크기)\n",
    ">- pyr_scale: Image Pyramid Scale\n",
    ">- levels: Number of image pyramids\n",
    ">- winsize: window size\n",
    ">- iterations: iterations\n",
    ">- poly_n: Neighbor size for polynomial approximation, 5 or 7 \n",
    ">- poly_sigma: Gaussian sigma to use in polynomial approximations (poly_n=5 : sigma=1.1, poly_n=7 : sigma=1.5)\n",
    ">- flags: \n",
    ">>- cv2.OPTFLOW_USE_INITAL_FLOW: flow value as the initial value,  \n",
    ">>- cv2.OPTFLOW_FARNEBACK_GAUSSIAN: Use Gaussian filters instead of box filters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcOPticalFlowFarneback  (track_optical_farneback.py)\n",
    "\n",
    "import cv2, numpy as np\n",
    "\n",
    "def drawFlow(img,flow,step=16):\n",
    "    h,w = img.shape[:2]\n",
    "    idx_y,idx_x = np.mgrid[step/2:h:step,step/2:w:step].astype(np.int32) # Finding Grid Indexes 16 pixels apart\n",
    "    indices =  np.stack( (idx_x,idx_y), axis =-1).reshape(-1,2)\n",
    "\n",
    "    for x,y in indices: \n",
    "        cv2.circle(img, (x,y), 1, (0,255,0), -1) \n",
    "        dx,dy = flow[y, x].astype(np.int32)            # Travel distance\n",
    "        cv2.line(img, (x,y), (x+dx, y+dy), (0,255, 0),2, cv2.LINE_AA )\n",
    "\n",
    "prev = None \n",
    "\n",
    "cap = cv2.VideoCapture('./Videos/walking2.mp4') # walking2.mp4 shopping_mall_walking.mp4\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "delay = int(100/fps)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if not ret: break\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    if prev is None: \n",
    "        prev = gray \n",
    "    else:\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev,gray,None, 0.5,3,15,3,5,1.1,cv2.OPTFLOW_FARNEBACK_GAUSSIAN) \n",
    "        drawFlow(frame,flow)  \n",
    "        prev = gray  \n",
    "\n",
    "    cv2.imshow('OpticalFlow-Farneback', frame)\n",
    "    if cv2.waitKey(delay) == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The drawFlow() function draws grid-shaped dots 16 pixels apart.   \n",
    "- At each point, the corresponding pixels were displayed as a line as they moved.   \n",
    "- Unlike sparse optical flows, dense optical flows can detect movement throughout the image.  \n",
    "\n",
    "        drawFlow() 함수로 16픽셀 간격으로 격자 모양의 점을 찍었습니다.   \n",
    "        각 점에서 해당하는 픽셀이 이동한 만큼 선으로 표시했습니다.   \n",
    "        희소 광학 흐름과 다르게 밀집 광학 흐름은 영상 전체에서 일어나는 움직임을 감지할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2 : Sparse Optical Flow w/Shi-tomasi corner detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2 : Optical Flow w/Shi-tomasi corner detection\n",
    "import cv2\n",
    "import numpy as np\n",
    "# Parameters for Shi-tomasi corner detection\n",
    "st_params = dict(maxCorners = 30,\n",
    "                qualityLevel = 0.2,\n",
    "                minDistance = 2,\n",
    "                blockSize = 7)\n",
    "\n",
    "cap = cv2.VideoCapture('./Videos/run.mp4')\n",
    "color = (0, 255, 0)\n",
    "ret, first_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev = cv2.goodFeaturesToTrack(prev_gray, mask = None, **st_params) # Find the strongest corners in the first frame\n",
    "mask = np.zeros_like(first_frame)  # Create an image with the same dimensions as the frame for later drawing purposes\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    next, status, error = cv2.calcOpticalFlowPyrLK(prev_gray, gray, prev, None, **lk_params) # Calculate optical flow by Lucas-Kanade\n",
    "    good_old = prev[status == 1]   # Select good feature for the previous position\n",
    "    good_new = next[status == 1]   # Select good feature for the next position\n",
    "    \n",
    "    for i, (new,old) in enumerate(zip(good_new,good_old)): # Draw optical flow track\n",
    "        a, b = new.ravel()                                 # Return coordinates for the new point\n",
    "        c, d = new.ravel()                                 # Return coordinates for the old point\n",
    "#         print(a,b,c,d)\n",
    "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color, 2)    # Draw line between new and old position\n",
    "        frame = cv2.circle(frame, (int(a), int(b)), 3, color, -1)    # Draw filled circle\n",
    "   \n",
    "    output = cv2.add(frame, mask)      # Overlay optical flow on original frame\n",
    "    prev_gray = gray.copy()            # Update previous frame\n",
    "    prev = good_new.reshape(-1, 1, 2)  # Update previous good features\n",
    "    \n",
    "    cv2.imshow(\"Optical Flow\", output)\n",
    "    \n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV-py36",
   "language": "python",
   "name": "cv_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
