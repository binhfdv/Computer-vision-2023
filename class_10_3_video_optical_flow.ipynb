{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ğŸ‘‰ class_10_3 Â» _Optical Flow_ </center>\n",
    "\n",
    "## Optical Flow (ê´‘í•™ íë¦„)  \n",
    "\n",
    "Optical flow refers to __the movement pattern of an object__ in an image.   \n",
    "The distribution of the direction and distance traveled by the pixels between the previous and next frames.   \n",
    "With optical flow, __you can see how much and in which direction an object has moved in the image.__   \n",
    "In addition, additional calculations can be performed to__ predict the behavior of objects__.  \n",
    "\n",
    "        ê´‘í•™ íë¦„ì´ë€ ì˜ìƒ ë‚´ ë¬¼ì²´ì˜ ì›€ì§ì„ íŒ¨í„´ì„ ë§í•©ë‹ˆë‹¤.   \n",
    "        ì´ì „ í”„ë ˆì„ê³¼ ë‹¤ìŒ í”„ë ˆì„ ê°„ í”½ì…€ì´ ì´ë™í•œ ë°©í–¥ê³¼ ê±°ë¦¬ ë¶„í¬ì…ë‹ˆë‹¤.   \n",
    "        ê´‘í•™ íë¦„ìœ¼ë¡œ ì˜ìƒ ë‚´ ë¬¼ì²´ê°€ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ì›€ì§ì˜€ëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
    "        ë”ë¶ˆì–´ ì¶”ê°€ ì—°ì‚°ì„ í•˜ë©´ ë¬¼ì²´ì˜ ì›€ì§ì„ì„ ì˜ˆì¸¡í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<img src='./images/practice_img/op_flow.jpeg' width=500 height=200>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Optical_flow\n",
    "\n",
    "Optical flow assumes two facts:  \n",
    "1. The pixel intensity of moving objects between consecutive frames remains unchanged.    \n",
    "2. Neighboring pixels have similar movements.  \n",
    "\n",
    "        ê´‘í•™ íë¦„ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ì‚¬ì‹¤ì„ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "        1. ì—°ì†ëœ í”„ë ˆì„ ì‚¬ì´ì—ì„œ ì›€ì§ì´ëŠ” ë¬¼ì²´ì˜ í”½ì…€ ê°•ë„(intensity)ëŠ” ë³€í•¨ì´ ì—†ë‹¤.  \n",
    "        2. ì´ì›ƒí•˜ëŠ” í”½ì…€ì€ ë¹„ìŠ·í•œ ì›€ì§ì„ì„ ê°–ëŠ”ë‹¤.  \n",
    "        \n",
    "There are two ways to calculate optical flow.   \n",
    "- __Sparse optical flow__: only some pixels are counted, __Lucas-Kanade Algorithm__ \n",
    "- __Dense optical flow__: Counts all pixels in the image, __Gunner Farneback Algorithm__  \n",
    "    \n",
    "        ê´‘í•™ íë¦„ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ì…ë‹ˆë‹¤.   \n",
    "        - í¬ì†Œ(sparse) ê´‘í•™ íë¦„ : ì¼ë¶€ í”½ì…€ë§Œ ê³„ì‚°, Lucas-Kanade Algorithm  \n",
    "        - ë°€ì§‘(dense) ê´‘í•™ íë¦„ : ì˜ìƒ ì „ì²´ í”½ì…€ì„ ëª¨ë‘ ê³„ì‚°, Gunner Farneback Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ Lucas-Kanade Algorithm \n",
    "\n",
    "The Lucas-Kanade algorithm is an algorithm that uses __the assumption that neighboring pixels behave similarly.__  \n",
    "\n",
    "- The Lucas-Kanade algorithm uses a small window (3 x 3 patches) to calculate movement.  \n",
    "- Due to the small size of the window, large object movement will cause problems.  \n",
    ">- Use an image pyramid to improve this problem. \n",
    ">- The farther up the image pyramid (the smaller the image), the less visible the small movement and the smaller the small movement.   \n",
    ">- It can detect even large movements like this.\n",
    "\n",
    "        ë£¨ì¹´ìŠ¤-ì¹´ë‚˜ë° ì•Œê³ ë¦¬ì¦˜ì€ ì´ì›ƒí•˜ëŠ” í”½ì…€ì´ ë¹„ìŠ·í•˜ê²Œ ì›€ì§ì¸ë‹¤ëŠ” ê°€ì •ì„ ì´ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.   \n",
    "        - ë£¨ì¹´ìŠ¤-ì¹´ë‚˜ë° ì•Œê³ ë¦¬ì¦˜ì€ ì‘ì€ ìœˆë„(3 x 3 patch)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›€ì§ì„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
    "        - ìœˆë„ í¬ê¸°ê°€ ì‘ê¸° ë•Œë¬¸ì— ë¬¼ì²´ ì›€ì§ì„ì´ í¬ë©´ ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.  \n",
    "        - ì´ ë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ í”¼ë¼ë¯¸ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
    "        - ì´ë¯¸ì§€ í”¼ë¼ë¯¸ë“œ ìœ„ìª½ìœ¼ë¡œ ê°ˆìˆ˜ë¡(ì´ë¯¸ì§€ê°€ ì‘ì•„ì§ˆìˆ˜ë¡) ì‘ì€ ì›€ì§ì„ì€ í‹°ê°€ ì•ˆ ë‚˜ê³  í° ì›€ì§ì„ì€ ì‘ì€ ì›€ì§ì„ ê°™ì•„ ë³´ì…ë‹ˆë‹¤.   \n",
    "          ì´ë ‡ê²Œ í° ì›€ì§ì„ë„ ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "OpenCV **cv2.calcOpticalFlowPyrLK()** \n",
    "\n",
    "**nextPts, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, nextPts, status, err, wirnSize, maxLevel, criteria, flags, minEigThreshold)**  \n",
    "\n",
    ">- prevPts: prev Img cnr keypoints, use cv2.goodFeaturesToTrack()\n",
    ">- nextPst: next Img cnr keypoints,\n",
    ">- status:  Same length as nextPts, 1 if there is a correspondence point, 0 if it does not nextPtsì™€ ê°™ì€ ê¸¸ì´/ëŒ€ì‘ì ì´ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0\n",
    ">- err: correspondence point error\n",
    ">- winSize=(21,21): \n",
    ">- maxLevel=3: pyramid level\n",
    ">- criteria=(COUNT+EPS, 30, 0.01): Requirements to stop iterative exploration ë°˜ë³µ íƒìƒ‰ ì¤‘ì§€ ìš”ê±´  \n",
    ">>- cv2.TERM_CRITERIA_EPS: Stop if accuracy is less than epsilon ì •í™•ë„ê°€ epsilonë³´ë‹¤ ì‘ìœ¼ë©´ ì¤‘ì§€,   \n",
    ">>- cv2.TERM_CRITERIA_MAX_ITER: Stop when max_iter number of times is met max_iter íšŸìˆ˜ë¥¼ ì±„ìš°ë©´ ì¤‘ì§€,   \n",
    ">>- cv2.TERM_CRITERIA_COUNT: same as MAX_ITER\n",
    ">- flgs=0:   \n",
    ">>- 0: Using prevPts as the initial value of nextPts  prevPtsë¥¼ nextPtsì˜ ì´ˆê¸° ê°’ìœ¼ë¡œ ì‚¬ìš©,   \n",
    ">>- cv2.OPTFLOW_USE_INITAL_FLOW: Using prevPts as the initial value of nextPts nextPtsì˜ ê°’ì„ ì´ˆê¸° ê°’ìœ¼ë¡œ ì‚¬ìš©,   \n",
    ">>- cv2.OPTFLOW_LK_GET_MIN_EIGENVALS: Calculate error as the least unique value ì˜¤ì°¨ë¥¼ ìµœì†Œ ê³ ìœ  ê°’ìœ¼ë¡œ ê³„ì‚°  \n",
    ">- minEigThreshold=1e-4: Minimum threshold eigenvalue to use in calculating the correspondence point ëŒ€ì‘ì  ê³„ì‚°ì— ì‚¬ìš©í•  ìµœì†Œ ì„ê³„ ê³ ìœ  ê°’\n",
    "\n",
    "    \n",
    "The above function does not count all the pixels in the video at once. \n",
    "- Calculate using only feature points obtained from the cv2.goodFeaturesToTrack() function.   \n",
    "- The prevImg and nextImg parameters can be used to pass the previous and next frames.   \n",
    "- The prevPts parameter passes the features from the previous frame.   \n",
    "- This calculates where the feature point moved in the next frame and returns it to nextPts.   \n",
    "- If the two features correspond to each other, the status variable will be 1, otherwise it will be 0.   \n",
    "- Also, if maxLevel=0 does not use image pyramids. \n",
    "\n",
    "        ìœ„ í•¨ìˆ˜ëŠ” ì˜ìƒ ë‚´ í”½ì…€ ì „ì²´ë¥¼ í•œë²ˆì— ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
    "        - cv2.goodFeaturesToTrack() í•¨ìˆ˜ë¡œ ì–»ì€ íŠ¹ì§•ì ë§Œ í™œìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.   \n",
    "        - prevImgì™€ nextImg íŒŒë¼ë¯¸ã„·í„°ì—ëŠ” ì´ì „ í”„ë ˆì„ê³¼ ë‹¤ìŒ í”„ë ˆì„ì„ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.   \n",
    "        - prevPts íŒŒë¼ë¯¸í„°ì—ëŠ” ì´ì „ í”„ë ˆì„ì—ì„œ ì–»ì€ íŠ¹ì§•ì ì„ ì „ë‹¬í•©ë‹ˆë‹¤.   \n",
    "        - ê·¸ëŸ¬ë©´ íŠ¹ì§•ì ì´ ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ì–´ë””ë¡œ ì´ë™í–ˆëŠ”ì§€ ê³„ì‚°í•˜ì—¬ nextPtsë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.   \n",
    "        - ë‘ íŠ¹ì§•ì ì´ ì„œë¡œ ëŒ€ì‘í•˜ë©´ status ë³€ìˆ˜ê°€ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì´ ë©ë‹ˆë‹¤.   \n",
    "        - ë˜í•œ, maxLevel=0ì´ë©´ ì´ë¯¸ì§€ í”¼ë¼ë¯¸ë“œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lucas-Kanade Algorithm calcOpticalFlowPyrLK (track_opticalLK.py)\n",
    "\n",
    "import numpy as np, cv2\n",
    "\n",
    "cap = cv2.VideoCapture('./Videos/walking2.mp4')\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "delay = int(500/fps)\n",
    "\n",
    "color = np.random.randint(0,255,(200,3)) # random color\n",
    "lines = None    \n",
    "prevImg = None  \n",
    "\n",
    "termcriteria =  (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03) # calcOpticalFlowPyrLK stop criteria\n",
    "# Parameters for Lucas-Kande optical flow\n",
    "# lk_params = dict(winSize = (15,15), maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10,1))\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    if prevImg is None:\n",
    "        prevImg = gray\n",
    "        lines = np.zeros_like(frame) \n",
    "        prevPt = cv2.goodFeaturesToTrack(prevImg, 200, 0.01, 10) # starting cnr detect\n",
    "    else:\n",
    "        nextImg = gray\n",
    "        nextPt, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, \\\n",
    "                                        prevPt, None, criteria=termcriteria)  # next cnr detect\n",
    "        prevMv = prevPt[status==1]\n",
    "        nextMv = nextPt[status==1]\n",
    "        for i,(p, n) in enumerate(zip(prevMv, nextMv)):\n",
    "            px,py = p.ravel()\n",
    "            nx,ny = n.ravel()\n",
    "            cv2.line(lines, (int(px), int(py)), (int(nx), int(ny)), color[i].tolist(), 2) \n",
    "            cv2.circle(img_draw, (int(nx), int(ny)), 2, color[i].tolist(), -1) \n",
    "        img_draw = cv2.add(img_draw, lines) \n",
    "        prevImg = nextImg \n",
    "        prevPt = nextMv.reshape(-1,1,2)\n",
    "\n",
    "    cv2.imshow('OpticalFlow-LK', img_draw)\n",
    "    key = cv2.waitKey(delay)\n",
    "    if key == 27 : \n",
    "        break\n",
    "    elif key == 8:       # Backspace:remove tracking history\n",
    "        prevImg = None\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cv2.goodFeatureToTrack() :  Detects the features of the previous frame   \n",
    "- cv2.calcOpticalFlowPyrLK() : Detects the features of the next frame   \n",
    "- Select only the best matching features from the previous frame and the next frame, and display them as lines and dots.     \n",
    "- Composite a tracking line into the original image to display the tracking line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ Gunner Farneback Algorithm\n",
    "\n",
    "The Gunner Farneback Algorithm is __a dense algorithm that calculates optical flow using every pixel in an image.__   \n",
    "- Dense optical flow, unlike sparse optical flow, is calculated using the entire pixel of the image.   \n",
    "- So you don't have to pass on features to track.   \n",
    "- However, it is slow because it is calculated using all pixels.  \n",
    "\n",
    "        - Gunner Farneback Algorithmì€ ì˜ìƒ ì „ì²´ì˜ í”½ì…€ì„ í™œìš©í•´ ê´‘í•™ íë¦„ì„ ê³„ì‚°í•˜ëŠ” ë°€ì§‘ ë°©ì‹ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.   \n",
    "        - ë°€ì§‘ ê´‘í•™ íë¦„ì€ í¬ì†Œ ê´‘í•™ íë¦„ê³¼ ë‹¤ë¥´ê²Œ ì˜ìƒ ì „ì²´ í”½ì…€ì„ í™œìš©í•´ ê³„ì‚°í•©ë‹ˆë‹¤.   \n",
    "        - ê·¸ë˜ì„œ ì¶”ì í•  íŠ¹ì§•ì ì„ ë”°ë¡œ ì „ë‹¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì „ì²´ í”½ì…€ì„ í™œìš©í•´ ê³„ì‚°í•˜ë¯€ë¡œ ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤.  \n",
    "\n",
    "OpenCV \n",
    "\n",
    "- **flow = cv2.calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)**\n",
    ">- prev, next: prev, next frame\n",
    ">- flow: Optical flow calculations show that each pixel travels (the same size as the input) ê´‘í•™ íë¦„ ê³„ì‚° ê²°ê³¼, ê° í”½ì…€ì´ ì´ë™í•œ ê±°ë¦¬ (ì…ë ¥ê³¼ ë™ì¼í•œ í¬ê¸°)\n",
    ">- pyr_scale: Image Pyramid Scale\n",
    ">- levels: Number of image pyramids\n",
    ">- winsize: window size\n",
    ">- iterations: iterations\n",
    ">- poly_n: Neighbor size for polynomial approximation, 5 or 7 \n",
    ">- poly_sigma: Gaussian sigma to use in polynomial approximations (poly_n=5 : sigma=1.1, poly_n=7 : sigma=1.5)\n",
    ">- flags: \n",
    ">>- cv2.OPTFLOW_USE_INITAL_FLOW: flow value as the initial value,  \n",
    ">>- cv2.OPTFLOW_FARNEBACK_GAUSSIAN: Use Gaussian filters instead of box filters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcOPticalFlowFarneback  (track_optical_farneback.py)\n",
    "\n",
    "import cv2, numpy as np\n",
    "\n",
    "def drawFlow(img,flow,step=16):\n",
    "    h,w = img.shape[:2]\n",
    "    idx_y,idx_x = np.mgrid[step/2:h:step,step/2:w:step].astype(np.int32) # Finding Grid Indexes 16 pixels apart\n",
    "    indices =  np.stack( (idx_x,idx_y), axis =-1).reshape(-1,2)\n",
    "\n",
    "    for x,y in indices: \n",
    "        cv2.circle(img, (x,y), 1, (0,255,0), -1) \n",
    "        dx,dy = flow[y, x].astype(np.int32)            # Travel distance\n",
    "        cv2.line(img, (x,y), (x+dx, y+dy), (0,255, 0),2, cv2.LINE_AA )\n",
    "\n",
    "prev = None \n",
    "\n",
    "cap = cv2.VideoCapture('./Videos/walking2.mp4') # walking2.mp4 shopping_mall_walking.mp4\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "delay = int(100/fps)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if not ret: break\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    if prev is None: \n",
    "        prev = gray \n",
    "    else:\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev,gray,None, 0.5,3,15,3,5,1.1,cv2.OPTFLOW_FARNEBACK_GAUSSIAN) \n",
    "        drawFlow(frame,flow)  \n",
    "        prev = gray  \n",
    "\n",
    "    cv2.imshow('OpticalFlow-Farneback', frame)\n",
    "    if cv2.waitKey(delay) == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The drawFlow() function draws grid-shaped dots 16 pixels apart.   \n",
    "- At each point, the corresponding pixels were displayed as a line as they moved.   \n",
    "- Unlike sparse optical flows, dense optical flows can detect movement throughout the image.  \n",
    "\n",
    "        drawFlow() í•¨ìˆ˜ë¡œ 16í”½ì…€ ê°„ê²©ìœ¼ë¡œ ê²©ì ëª¨ì–‘ì˜ ì ì„ ì°ì—ˆìŠµë‹ˆë‹¤.   \n",
    "        ê° ì ì—ì„œ í•´ë‹¹í•˜ëŠ” í”½ì…€ì´ ì´ë™í•œ ë§Œí¼ ì„ ìœ¼ë¡œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤.   \n",
    "        í¬ì†Œ ê´‘í•™ íë¦„ê³¼ ë‹¤ë¥´ê²Œ ë°€ì§‘ ê´‘í•™ íë¦„ì€ ì˜ìƒ ì „ì²´ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì›€ì§ì„ì„ ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2 : Sparse Optical Flow w/Shi-tomasi corner detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2 : Optical Flow w/Shi-tomasi corner detection\n",
    "import cv2\n",
    "import numpy as np\n",
    "# Parameters for Shi-tomasi corner detection\n",
    "st_params = dict(maxCorners = 30,\n",
    "                qualityLevel = 0.2,\n",
    "                minDistance = 2,\n",
    "                blockSize = 7)\n",
    "\n",
    "cap = cv2.VideoCapture('./Videos/run.mp4')\n",
    "color = (0, 255, 0)\n",
    "ret, first_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev = cv2.goodFeaturesToTrack(prev_gray, mask = None, **st_params) # Find the strongest corners in the first frame\n",
    "mask = np.zeros_like(first_frame)  # Create an image with the same dimensions as the frame for later drawing purposes\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    next, status, error = cv2.calcOpticalFlowPyrLK(prev_gray, gray, prev, None, **lk_params) # Calculate optical flow by Lucas-Kanade\n",
    "    good_old = prev[status == 1]   # Select good feature for the previous position\n",
    "    good_new = next[status == 1]   # Select good feature for the next position\n",
    "    \n",
    "    for i, (new,old) in enumerate(zip(good_new,good_old)): # Draw optical flow track\n",
    "        a, b = new.ravel()                                 # Return coordinates for the new point\n",
    "        c, d = new.ravel()                                 # Return coordinates for the old point\n",
    "#         print(a,b,c,d)\n",
    "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color, 2)    # Draw line between new and old position\n",
    "        frame = cv2.circle(frame, (int(a), int(b)), 3, color, -1)    # Draw filled circle\n",
    "   \n",
    "    output = cv2.add(frame, mask)      # Overlay optical flow on original frame\n",
    "    prev_gray = gray.copy()            # Update previous frame\n",
    "    prev = good_new.reshape(-1, 1, 2)  # Update previous good features\n",
    "    \n",
    "    cv2.imshow(\"Optical Flow\", output)\n",
    "    \n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV-py36",
   "language": "python",
   "name": "cv_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
