{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 👉 class_10_4 Video » _Object Tracking_ </center>\n",
    "\n",
    "## Object Tracking \n",
    "\n",
    "Simply put, locating an object in successive frames of a video is called tracking.  \n",
    "The definition sounds straight forward but in computer vision and machine learning, tracking is a very broad term that encompasses conceptually similar but technically different ideas.   \n",
    "For example, all the following different but related ideas are generally studied under __Object Tracking__  \n",
    "\n",
    "        간단히 말해서 비디오의 연속 프레임에서 개체를 찾는 것을 추적이라고 합니다.  \n",
    "        정의는 간단해 보이지만 컴퓨터 비전과 기계 학습에서 추적은 개념적으로 유사하지만 기술적으로 다른 아이디어를 포괄하는 매우 광범위한 용어입니다.   \n",
    "        예를 들어, 다음과 같지만 관련된 아이디어는 모두 일반적으로 객체 추적에서 연구됩니다\n",
    "        \n",
    "* **Dense Optical flow:** These algorithms help estimate the motion vector of every pixel in a video frame.\n",
    "* **Sparse optical flow:** These algorithms, like the Kanade-Lucas-Tomashi (KLT) feature tracker, track the location of a few feature points in an image.\n",
    "* **Kalman Filtering:** A very popular signal processing algorithm used to predict the location of a moving object based on prior motion information.   \n",
    ">- One of the early applications of this algorithm was missile guidance!   \n",
    ">- Also \"the on-board computer that guided the descent of the Apollo 11 lunar module to the moon had a Kalman filter\".  \n",
    "* **Meanshift and Camshift:** These are algorithms for locating the maxima of a density function. They are also used for tracking.\n",
    "* **Single object trackers:** In this class of trackers, the first frame is marked using a rectangle to indicate the location of the object we want to track.   \n",
    "The object is then tracked in subsequent frames using the tracking algorithm.   \n",
    "In most real-life applications, these trackers are used in conjunction with an object detector.\n",
    "* **Multiple object track finding algorithms:** In cases when we have a fast object detector, it makes sense to detect multiple objects in each frame and then run a track finding algorithm that identifies which rectangle in one frame corresponds to a rectangle in the next frame.\n",
    ">- Multiple Object Tracking has come a long way.   \n",
    ">- It uses object detection and novel motion prediction algorithms to get accurate tracking information.   \n",
    ">- For example, DeepSort uses the YOLO network to get blazing-fast inference speed. It is based on SORT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object tracking using OpenCV 4 – the Tracking API  \n",
    "\n",
    "OpenCV 4 comes with a tracking API that contains implementations of many single object tracking algorithms.   \n",
    "There are 8 different trackers available in OpenCV 4.2:\n",
    "* **BOOSTING, MIL, KCF, TLD, MEDIANFLOW, GOTURN, MOSSE, and CSRT.**   \n",
    "  \n",
    "- OpenCV 3.2 has implementations of these 6 trackers — BOOSTING, MIL, TLD, MEDIANFLOW, MOSSE, and GOTURN.   \n",
    "- OpenCV 3.1 has implementations of these 5 trackers — BOOSTING, MIL, KCF, TLD, MEDIANFLOW.   \n",
    "- OpenCV 3.0 has implementations of the following 4 trackers — BOOSTING, MIL, TLD, MEDIANFLOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object tracking using OpenCV – the Algorithms  \n",
    "\n",
    "Let's begin by first explaining some general principles behind tracking.   \n",
    "- In tracking, our goal is to find an object in the current frame given we have tracked the object successfully in all (or nearly all) previous frames.  \n",
    "- Since we have tracked the object up until the current frame, we know how it has been moving.   \n",
    "- In other words, we know the parameters of the motion model.   \n",
    ">- The motion model is just a fancy way of saying that you know the location and the velocity (speed + direction of motion) of the object in previous frames.   \n",
    ">- If you knew nothing else about the object, you could predict the new location based on the current motion model, and you would be pretty close to where the new location of the object is.  \n",
    ">- But we have more information than just the motion of the object.   \n",
    "- We know how the object looks in each of the previous frames.   \n",
    "- In other words, we can build an appearance model that encodes what the object looks like.   \n",
    "- This appearance model can be used to search in a small neighborhood of the location predicted by the motion model to more accurately predict the location of the object.  \n",
    "- The motion model predicts the approximate location of the object.   \n",
    "- The appearance model fine tunes this estimate to provide a more accurate estimate based on appearance.  \n",
    "- If the object was very simple and did not change it’s appearance much, we could use a simple template as an appearance model and look for that template.   \n",
    "- However, real life is not that simple. The appearance of an object can change dramatically.   \n",
    "- To tackle this problem, in many modern trackers, this appearance model is a classifier that is trained in an online manner.   \n",
    "\n",
    "        먼저 추적의 몇 가지 일반적인 원칙을 설명하는 것부터 시작하겠습니다.\n",
    "        - 추적에서 우리의 목표는 모든(또는 거의 모든) 이전 프레임에서 객체를 성공적으로 추적한 경우 현재 프레임에서 객체를 찾는 것입니다.\n",
    "        - 현재 프레임까지 물체를 추적했기 때문에 물체가 어떻게 움직이고 있는지 알 수 있습니다.\n",
    "        - 즉, 모션 모델의 매개변수를 알고 있습니다.\n",
    "        >- 모션 모델은 이전 프레임에서 객체의 위치와 속도(속도 + 모션 방향)를 알고 있다는 것을 표현하는 멋진 방법입니다.\n",
    "        >- 물체에 대해 아무것도 모른다면 현재 모션 모델을 기반으로 새 위치를 예측할 수 있으며 물체의 새 위치에 꽤 가깝습니다.\n",
    "        >- 하지만 우리는 물체의 움직임보다 더 많은 정보를 갖고 있습니다.\n",
    "        - 우리는 이전 프레임 각각에서 객체가 어떻게 보이는지 알고 있습니다.\n",
    "        - 즉, 객체의 모습을 인코딩하는 외관 모델을 구축할 수 있습니다.\n",
    "        - 이 외관 모델은 모션 모델이 예측한 위치의 작은 주변 지역을 검색하여 물체의 위치를 보다 정확하게 예측하는 데 사용할 수 있습니다.\n",
    "        - 모션 모델은 물체의 대략적인 위치를 예측합니다.\n",
    "        - 모양 모델은 이 추정치를 미세 조정하여 모양을 기반으로 보다 정확한 추정치를 제공합니다.\n",
    "        - 객체가 매우 단순하고 외관이 많이 변하지 않은 경우 간단한 템플릿을 외관 모델로 사용하고 해당 템플릿을 찾을 수 있습니다.\n",
    "        - 하지만 실제 생활은 그렇게 간단하지 않습니다. 물체의 모양은 극적으로 바뀔 수 있습니다.\n",
    "        - 이 문제를 해결하기 위해 많은 최신 추적기에서 이 모양 모델은 온라인 방식으로 훈련된 분류자입니다.  \n",
    "        \n",
    "The job of the classifier is to classify a rectangular region of an image as either an object or background.   \n",
    "The classifier takes in an image patch as input and returns a score between 0 and 1 to indicate the probability that the image patch contains the object.   \n",
    "The score is 0 when it is absolutely sure the image patch is the background and 1 when it is absolutely sure the patch is the object.  \n",
    "\n",
    "In machine learning, we use the word “online” to refer to algorithms that are trained on the fly at run time.   \n",
    "An offline classifier may need thousands of examples to train a classifier, but an online classifier is typically trained using very few examples at run time.  \n",
    "\n",
    "A classifier is trained by feeding it positive (object) and negative (background) examples.   \n",
    "If you want to build a classifier for detecting cats, you train it with thousands of images containing cats and thousands of images that do not contain cats.   \n",
    "This way the classifier learns to differentiate what is a cat and what is not.   \n",
    "While building an online classifier, we do not have the luxury of having thousands of examples of the positive and negative classes.  \n",
    "\n",
    "        분류자의 역할은 이미지의 직사각형 영역을 객체 또는 배경으로 분류하는 것입니다.\n",
    "        분류기는 이미지 패치를 입력으로 취하고 0에서 1 사이의 점수를 반환하여 이미지 패치에 객체가 포함될 확률을 나타냅니다.\n",
    "        이미지 패치가 배경이라고 절대적으로 확신할 때 점수는 0이고, 패치가 객체라고 절대적으로 확신할 때 점수는 1입니다.\n",
    "\n",
    "        기계 학습에서는 \"온라인\"이라는 단어를 사용하여 런타임에 즉석에서 훈련되는 알고리즘을 나타냅니다.\n",
    "        오프라인 분류자는 분류자를 훈련시키기 위해 수천 개의 예가 필요할 수 있지만 온라인 분류자는 일반적으로 런타임에 매우 적은 수의 예를 사용하여 훈련됩니다.\n",
    "\n",
    "        분류자는 긍정적(객체) 및 부정적(배경) 예시를 제공하여 훈련됩니다.\n",
    "        고양이를 감지하기 위한 분류기를 구축하려면 고양이가 포함된 수천 개의 이미지와 고양이가 포함되지 않은 수천 개의 이미지로 분류기를 훈련해야 합니다.\n",
    "        이런 방식으로 분류자는 고양이인지 아닌지를 구별하는 방법을 학습합니다.\n",
    "        온라인 분류기를 구축하는 동안 우리는 긍정적인 클래스와 부정적인 클래스의 수천 가지 예를 가질 여유가 없습니다.\n",
    "\n",
    "### ● BOOSTING Tracker  \n",
    "\n",
    "This tracker is based on an online version of AdaBoost — the algorithm that the HAAR cascade based face detector uses internally. This classifier needs to be trained at runtime with positive and negative examples of the object. The initial bounding box supplied by the user ( or by another object detection algorithm ) is taken as a positive example for the object, and many image patches outside the bounding box are treated as the background.  \n",
    "Given a new frame, the classifier is run on every pixel in the neighborhood of the previous location and the score of the classifier is recorded. The new location of the object is the one where the score is maximum. So now we have one more positive example for the classifier.   \n",
    "As more frames come in, the classifier is updated with this additional data.\n",
    "- Pros: None. This algorithm is a decade old and works ok, but I could not find a good reason to use it especially when other advanced trackers (MIL, KCF) based on similar principles are available.  \n",
    "- Cons: Tracking performance is mediocre. It does not reliably know when tracking has failed.  \n",
    "    \n",
    "        이 추적기는 HAAR 캐스케이드 기반 얼굴 감지기가 내부적으로 사용하는 알고리즘인 AdaBoost의 온라인 버전을 기반으로 합니다. 이 분류자는 객체의 긍정적인 예와 부정적인 예를 사용하여 런타임에 훈련되어야 합니다. 사용자(또는 다른 객체 감지 알고리즘)가 제공한 초기 경계 상자는 객체에 대한 긍정적인 예로 간주되며 경계 상자 외부의 많은 이미지 패치는 배경으로 처리됩니다.\n",
    "        새로운 프레임이 주어지면 분류기는 이전 위치 근처의 모든 픽셀에서 실행되고 분류기의 점수가 기록됩니다. 개체의 새 위치는 점수가 최대인 위치입니다. 이제 분류기에 대한 긍정적인 예가 하나 더 있습니다.\n",
    "        더 많은 프레임이 들어오면 분류자는 이 추가 데이터로 업데이트됩니다.\n",
    "        - 장점: 없음. 이 알고리즘은 10년이 지났고 잘 작동하지만 특히 유사한 원리를 기반으로 하는 다른 고급 추적기(MIL, KCF)를 사용할 수 있는 경우 이 알고리즘을 사용해야 할 타당한 이유를 찾을 수 없습니다.\n",
    "        - 단점: 추적 성능이 평범합니다. 추적이 언제 실패했는지 확실하게 알 수 없습니다.\n",
    "            \n",
    "### ● MIL Tracker  \n",
    "This tracker is similar in idea to the BOOSTING tracker described above.   \n",
    "The big difference is that instead of considering only the current location of the object as a positive example, it looks in a small neighborhood around the current location to generate several potential positive examples.   \n",
    "You may be thinking that it is a bad idea because in most of these “positive” examples the object is not centered.\n",
    "This is where Multiple Instance Learning (MIL) comes to rescue. In MIL, you do not specify positive and negative examples, but positive and negative “bags”.  \n",
    "The collection of images in the positive bag are not all positive examples. Instead, only one image in the positive bag needs to be a positive example!  \n",
    "In our example, a positive bag contains the patch centered on the current location of the object and also patches in a small neighborhood around it.   \n",
    "Even if the current location of the tracked object is not accurate, when samples from the neighborhood of the current location are put in the positive bag, there is a good chance that this bag contains at least one image in which the object is nicely centered.   \n",
    "MIL project page has more information for people who like to dig deeper into the inner workings of the MIL tracker.\n",
    "- Pros: The performance is pretty good. It does not drift as much as the BOOSTING tracker and it does a reasonable job under partial occlusion. If you are using OpenCV 3.0, this might be the best tracker available to you. But if you are using a higher version, consider KCF.\n",
    "- Cons: Tracking failure is not reported reliably. Does not recover from full occlusion.  \n",
    "    \n",
    "        이 추적기는 위에서 설명한 BOOSTING 추적기와 아이디어가 유사합니다.\n",
    "        가장 큰 차이점은 객체의 현재 위치만을 긍정적인 예로 고려하는 것이 아니라, 현재 위치 주변의 작은 동네를 살펴보며 여러 가지 잠재적인 긍정적인 사례를 생성한다는 점입니다.\n",
    "        대부분의 \"긍정적인\" 예에서 객체가 중앙에 있지 않기 때문에 이것이 나쁜 생각이라고 생각할 수도 있습니다.\n",
    "        이것이 다중 인스턴스 학습(MIL)이 구출되는 곳입니다. MIL에서는 긍정적인 예와 부정적인 예를 지정하는 것이 아니라 긍정적인 예와 부정적인 \"백\"을 지정합니다.\n",
    "        포지티브 백에 담긴 이미지 모음이 모두 포지티브 예시는 아닙니다. 대신, 포지티브 백에 있는 이미지 하나만 포지티브 예시가 되어야 합니다!\n",
    "        이 예에서 포지티브 백에는 객체의 현재 위치를 중심으로 하는 패치와 객체 주변의 작은 이웃에 있는 패치도 포함되어 있습니다.\n",
    "        추적된 객체의 현재 위치가 정확하지 않더라도 현재 위치 근처의 샘플을 포지티브 백에 넣으면 이 백에 객체가 중앙에 잘 배치된 이미지가 하나 이상 포함될 가능성이 높습니다.\n",
    "        MIL 프로젝트 페이지에는 MIL 추적기의 내부 작동 방식을 더 깊이 파고드는 사람들을 위한 더 많은 정보가 있습니다.\n",
    "        - 장점 : 성능이 꽤 좋습니다. BOOSTING 추적기만큼 드리프트하지 않으며 부분 폐쇄 하에서 합리적인 작업을 수행합니다. OpenCV 3.0을 사용하고 있다면 이것이 최고의 트래커일 수 있습니다. 그러나 더 높은 버전을 사용하는 경우 KCF를 고려하십시오.\n",
    "        - 단점: 추적 실패가 안정적으로 보고되지 않습니다. 완전 폐쇄에서 회복되지 않습니다.  \n",
    "    \n",
    "### ● KCF Tracker\n",
    "KFC stands for Kernelized Correlation Filters. This tracker builds on the ideas presented in the previous two trackers. This tracker utilizes the fact that the multiple positive samples used in the MIL tracker have large overlapping regions. This overlapping data leads to some nice mathematical properties that are exploited by this tracker to make tracking faster and more accurate at the same time.\n",
    "- Pros: Accuracy and speed are both better than MIL and it reports tracking failure better than BOOSTING and MIL.   \n",
    "If you are using OpenCV 3.1 and above, I recommend using this for most applications.\n",
    "- Cons: Does not recover from full occlusion. \n",
    "    \n",
    "        KFC는 커널화된 상관 필터(Kernelized Correlation Filter)를 나타냅니다. 이 추적기는 이전 두 추적기에서 제시된 아이디어를 기반으로 합니다. 이 추적기는 MIL 추적기에서 사용되는 여러 개의 양성 샘플이 큰 중첩 영역을 가지고 있다는 사실을 활용합니다. 이러한 중복되는 데이터는 이 추적기가 활용하여 동시에 더 빠르고 정확한 추적을 가능하게 하는 몇 가지 멋진 수학적 속성으로 이어집니다.\n",
    "        - 장점: 정확성과 속도 모두 MIL보다 우수하며 BOOSTING 및 MIL보다 추적 실패를 더 잘 보고합니다.\n",
    "        OpenCV 3.1 이상을 사용하는 경우 대부분의 애플리케이션에 이 버전을 사용하는 것이 좋습니다.\n",
    "        - 단점: 완전 폐쇄에서 회복되지 않습니다.  \n",
    "            \n",
    "### ● TLD Tracker\n",
    "TLD stands for Tracking, learning, and detection. As the name suggests, this tracker decomposes the long term tracking task into three components — (short term) tracking, learning, and detection. From the author’s paper, “The tracker follows the object from frame to frame. The detector localizes all appearances that have been observed so far and corrects the tracker if necessary.\n",
    "The learning estimates detector’s errors and updates it to avoid these errors in the future.” This output of this tracker tends to jump around a bit. For example, if you are tracking a pedestrian and there are other pedestrians in the scene, this tracker can sometimes temporarily track a different pedestrian than the one you intended to track. On the positive side, this track appears to track an object over a larger scale, motion, and occlusion. If you have a video sequence where the object is hidden behind another object, this tracker may be a good choice.\n",
    "- Pros: Works the best under occlusion over multiple frames. Also, tracks best over scale changes.\n",
    "- Cons: Lots of false positives making it almost unusable.  \n",
    "    \n",
    "TLD는 추적(Tracking), 학습(Learning), 탐지(Detection)를 의미합니다. 이름에서 알 수 있듯이 이 추적기는 장기 추적 작업을 (단기) 추적, 학습 및 탐지의 세 가지 구성 요소로 분해합니다. 저자의 논문에서 “추적기는 프레임마다 개체를 따라갑니다. 검출기는 지금까지 관찰된 모든 모양의 위치를 파악하고 필요한 경우 추적기를 수정합니다.\n",
    "학습은 감지기의 오류를 추정하고 이를 업데이트하여 향후 이러한 오류를 방지합니다.\" 이 추적기의 출력은 약간 점프하는 경향이 있습니다. 예를 들어, 보행자를 추적하고 있는데 장면에 다른 보행자가 있는 경우 이 추적기는 때때로 추적하려는 보행자가 아닌 다른 보행자를 일시적으로 추적할 수 있습니다. 긍정적인 측면에서 이 트랙은 더 큰 규모, 모션 및 폐색에 걸쳐 개체를 추적하는 것으로 보입니다. 개체가 다른 개체 뒤에 숨겨져 있는 비디오 시퀀스가 있는 경우 이 추적기가 좋은 선택일 수 있습니다.\n",
    "- 장점: 여러 프레임에 걸쳐 폐색된 상태에서 가장 잘 작동합니다. 또한 규모 변화에 대해 가장 잘 추적합니다.\n",
    "- 단점: 오탐(false positive)이 많아 거의 사용할 수 없습니다.  \n",
    "    \n",
    "### ● MEDIANFLOW Tracker\n",
    "Internally, this tracker tracks the object in both forward and backward directions in time and measures the discrepancies between these two trajectories.   \n",
    "Minimizing this ForwardBackward error enables them to reliably detect tracking failures and select reliable trajectories in video sequences.\n",
    "In my tests, I found this tracker works best when the motion is predictable and small.   \n",
    "Unlike, other trackers that keep going even when the tracking has clearly failed, this tracker knows when the tracking has failed.\n",
    "- Pros: Excellent tracking failure reporting. Works very well when the motion is predictable and there is no occlusion.  \n",
    "- Cons: Fails under large motion.  \n",
    "    \n",
    "        내부적으로 이 추적기는 시간에 따라 앞뒤 방향으로 물체를 추적하고 이 두 궤적 사이의 불일치를 측정합니다.\n",
    "        이 ForwardBackward 오류를 최소화하면 추적 실패를 안정적으로 감지하고 비디오 시퀀스에서 신뢰할 수 있는 궤적을 선택할 수 있습니다.\n",
    "        테스트에서 이 추적기는 움직임이 예측 가능하고 작을 때 가장 잘 작동하는 것으로 나타났습니다.\n",
    "        추적이 확실히 실패했음에도 계속 작동하는 다른 추적기와 달리 이 추적기는 추적이 실패한 시점을 알 수 있습니다.\n",
    "        - 장점: 추적 실패 보고가 뛰어납니다. 모션이 예측 가능하고 폐색이 없을 때 매우 잘 작동합니다.\n",
    "        - 단점: 큰 움직임에서는 실패합니다.  \n",
    "            \n",
    "### ● GOTURN tracker\n",
    "Out of all the tracking algorithms in the tracker class, this is the only one based on Convolutional Neural Network (CNN).   \n",
    "From OpenCV documentation, we know it is “robust to viewpoint changes, lighting changes, and deformations”.   \n",
    "But it does not handle occlusion very well.\n",
    "* GOTURN being a CNN based tracker, uses a Caffe model for tracking.   \n",
    "* The Caffe model and the proto text file must be present in the directory in which the code is present.   \n",
    "* These files can also be downloaded from the opencv_extra repository, concatenated, and extracted before use.\n",
    "* Update: GOTURN object tracking algorithm has been ported to OpenCV.\n",
    "\n",
    "### ● MOSSE tracker\n",
    "- Minimum Output Sum of Squared Error (MOSSE) uses an adaptive correlation for object tracking which produces stable correlation filters when initialized using a single frame.   \n",
    "- MOSSE tracker is robust to variations in lighting, scale, pose, and non-rigid deformations.   \n",
    "- It also detects occlusion based upon the peak-to-sidelobe ratio, which enables the tracker to pause and resume where it left off when the object reappears.   \n",
    "- MOSSE tracker also operates at a higher fps (450 fps and even more).   \n",
    "- To add to the positives, it is also very easy to implement, is as accurate as other complex trackers and much faster.   \n",
    "- But, on a performance scale, it lags behind the deep learning based trackers.  \n",
    "\n",
    "        - MOSSE(Minimum Output Sum of Squared Error)는 단일 프레임을 사용하여 초기화할 때 안정적인 상관 필터를 생성하는 개체 추적을 위한 적응형 상관 관계를 사용합니다.\n",
    "        - MOSSE 추적기는 조명, 규모, 포즈 및 비강성 변형의 변화에 강력합니다.\n",
    "        - 또한 피크 대 사이드로브 비율을 기반으로 폐색을 감지하므로 객체가 다시 나타날 때 추적기가 중단된 부분부터 일시 중지하고 다시 시작할 수 있습니다.\n",
    "        - MOSSE 추적기는 더 높은 fps(450fps 이상)에서도 작동합니다.\n",
    "        - 장점을 추가하자면, 구현하기가 매우 쉽고, 다른 복잡한 추적기만큼 정확하고 훨씬 빠릅니다.\n",
    "        - 하지만 성능면에서는 딥러닝 기반의 트래커에 비해 뒤떨어집니다.  \n",
    "        \n",
    "### ● CSRT tracker\n",
    "- In the Discriminative Correlation Filter with Channel and Spatial Reliability (DCF-CSR), we use the spatial reliability map for adjusting the filter support to the part of the selected region from the frame for tracking.   \n",
    "- This ensures enlarging and localization of the selected region and improved tracking of the non-rectangular regions or objects.   \n",
    "- It uses only 2 standard features (HoGs and Colornames).   \n",
    "- It also operates at a comparatively lower fps (25 fps) but gives higher accuracy for object tracking.   \n",
    "\n",
    "        - DCF-CSR(Discriminative Correlation Filter with Channel and Spatial Reliability)에서는 추적을 위해 프레임에서 선택한 영역의 일부에 대한 필터 지원을 조정하기 위해 공간 신뢰도 맵을 사용합니다.\n",
    "        - 이를 통해 선택한 영역의 확대 및 위치화가 보장되고 직사각형이 아닌 영역이나 개체에 대한 추적이 향상됩니다.\n",
    "        - 2가지 표준 기능(HoG 및 Colornames)만 사용합니다.\n",
    "        - 상대적으로 낮은 fps(25fps)에서도 작동하지만 객체 추적에 있어 더 높은 정확도를 제공합니다.      \n",
    "    \n",
    "In the commented code below we first set up the tracker by choosing a tracker type   \n",
    "- BOOSTING, MIL, KCF, TLD, MEDIANFLOW, GOTURN, MOSSE, or CSRT.   \n",
    "We then open a video and grab a frame.   \n",
    "We define a bounding box containing the object for the first frame and initialize the tracker with the first frame and the bounding box.   \n",
    "Finally, we read frames from the video and just update the tracker in a loop to obtain a new bounding box for the current frame.  \n",
    "Results are subsequently displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tracking API  \n",
    "\n",
    "OpenCV provides a Tracking API for tracking objects.   \n",
    "The Tracking API makes it easy to track objects.   \n",
    "The Tracking API constructor provided by OpenCV is shown below.   \n",
    "The constructor varies depending on the algorithm.  \n",
    "\n",
    "        OpenCV에서는 객체 추적을 위한 Tracking API를 제공합니다.   \n",
    "        Tracking API를 이용하면 쉽게 객체 추적을 할 수 있습니다.   \n",
    "        OpenCV에서 제공하는 Tracking API생성자는 아래와 같습니다.   \n",
    "        생성자는 알고리즘에 따라 다양합니다.\n",
    "\n",
    "* **tracker = cv2.TrackerBoosting_create():** Based on the AdaBoost algorithm\n",
    "* **tracker = cv2.TrackerMIL_create():** Based on the MIL(Multiple Instance Learning) algorithm\n",
    "* **tracker = cv2.TrackerKCF_create():** Based on KCF(Kernelized Correlation Filters) algorithm\n",
    "* **tracker = cv2.TrackerTLD_create():** Based on TLD(Tracking, Learning and Detection) algorithm\n",
    "* **tracker = cv2.TrackerMedianFlow_create():** Track the forward and reverse directions of objects to measure inconsistencies 객체의 전방향/역방향을 추적해서 불일치성을 측정\n",
    "* tracker = cv2.TrackerGOTURN_cretae(): Based on CNN(Convolutional Neural Networks)  not work\n",
    "* tracker = cv2.TrackerCSRT_create(): Based on CSRT(Channel and Spatial Reliability) not work\n",
    "* tracker = cv2.TrackerMOSSE_create(): Use grayscale internally not work\n",
    "\n",
    "- retval = cv2.Tracker.init(img, boundingBox): initialize Tracker \n",
    ">- img: input\n",
    ">- boundingBox: roi (x, y)\n",
    "\n",
    "- retval, boundingBox = cv2.Tracker.update(img): Find the location of the object to be tracked in a new frame\n",
    ">- img: new frame\n",
    ">- retval: track boolean\n",
    ">- boundingBox: new roi (x, y, w, h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you run the code, the screen will freeze.   \n",
    "- First, drag the object you want to track.   \n",
    "- Next, press the Space key. It will then play back on its own, and start tracking the object.   \n",
    "- You can press the keyboard 0~4 number keys to select the tracker algorithm.   \n",
    "- At the top of the screen, you will see 0:TrackerBoosting, 1:TrackerMIL, etc.   \n",
    "- This is an indication of which tracker you are currently using to track an object.\n",
    "\n",
    "코드를 처음 실행하면 화면이 멈춰있을 겁니다.   \n",
    "- 먼저 추적을 원하는 객체를 드래그합니다.   \n",
    "- 다음으로 스페이스(space) 키를 누릅니다. 그러면 알아서 재생되고, 객체 추적을 시작합니다.   \n",
    "- 키보드 0~4 숫자 키를 눌러 트랙커 알고리즘을 선택할 수 있습니다.   \n",
    "- 화면 상단의 0:TrackerBoosting, 1:TrackerMIL 등이 뜰 겁니다.   \n",
    "- 현재 어떤 트랙커로 객체 추적을 하고 있는지를 나타내는 표시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracker APIs (track_trackingAPI.py)\n",
    "\n",
    "import cv2\n",
    "\n",
    "trackers = [cv2.TrackerBoosting_create,  # trackers idx no = 0\n",
    "            cv2.TrackerMIL_create,       # trackers idx no = 1\n",
    "            cv2.TrackerKCF_create,       # trackers idx no = 2\n",
    "            cv2.TrackerTLD_create,       # trackers idx no = 3\n",
    "            cv2.TrackerMedianFlow_create,# trackers idx no = 4\n",
    "#             cv2.TrackerGOTURN_create, # not work\n",
    "#             cv2.TrackerCSRT_create,   # not work\n",
    "#             cv2.TrackerMOSSE_create   # not work\n",
    "           ]\n",
    "trackerIdx = 0     # trackers idx no\n",
    "tracker = None\n",
    "isFirst = True\n",
    "\n",
    "video_src_f = './Videos/car_drive.mp4'\n",
    "cap = cv2.VideoCapture(video_src_f)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "delay = int(1000/fps)\n",
    "win_name = 'Tracking APIs'\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Cannot read video file')\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    if tracker is None:            \n",
    "        cv2.putText(img_draw, \"Press the Space to set ROI!!\", \\\n",
    "            (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "    else:\n",
    "        ok, bbox = tracker.update(frame)   # set roi\n",
    "        (x,y,w,h) = bbox\n",
    "        if ok:                             # track\n",
    "            cv2.rectangle(img_draw, (int(x), int(y)), (int(x + w), int(y + h)), (0,255,0), 2, 1)\n",
    "        else : \n",
    "            cv2.putText(img_draw, \"Tracking fail.\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "    trackerName = tracker.__class__.__name__\n",
    "    cv2.putText(img_draw, str(trackerIdx) + \":\"+trackerName , (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0),2,cv2.LINE_AA)\n",
    "    cv2.imshow(win_name, img_draw)\n",
    "    key = cv2.waitKey(delay) & 0xff\n",
    "   \n",
    "    if key == ord(' ') or (video_src_f != 0 and isFirst):  # space bar\n",
    "        isFirst = False\n",
    "        roi = cv2.selectROI(win_name, frame, False)        # set roi\n",
    "        if roi[2] and roi[3]:        \n",
    "            tracker = trackers[trackerIdx]()    \n",
    "            isInit = tracker.init(frame, roi)\n",
    "    elif key in range(48, 56):                             # if tracker idx has changeed input 0~4\n",
    "        trackerIdx = key-48     \n",
    "        if bbox is not None:\n",
    "            tracker = trackers[trackerIdx]() \n",
    "            isInit = tracker.init(frame, bbox) \n",
    "    elif key == 27 : \n",
    "        break\n",
    "# else:\n",
    "#     print( \"Could not open video\")\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ▣ Single Object Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,! What tracker API would you like to use? \n",
      "Enter 0 for BOOSTING\n",
      "Enter 1 for MIL\n",
      "Enter 2 for KCF\n",
      "Enter 3 for TLD\n",
      "Enter 4 for MEDIANFLOW\n",
      "Enter 5 for GOTURN\n",
      "Enter 6 for MOSSE\n",
      "Enter 7 for CSRT\n",
      "Enter your tracker 4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def ask_for_tracker():\n",
    "    print('Hi,! What tracker API would you like to use? ')\n",
    "    print('Enter 0 for BOOSTING')\n",
    "    print('Enter 1 for MIL')\n",
    "    print('Enter 2 for KCF')\n",
    "    print('Enter 3 for TLD')\n",
    "    print('Enter 4 for MEDIANFLOW')\n",
    "    print('Enter 5 for GOTURN')\n",
    "    print('Enter 6 for MOSSE')\n",
    "    print('Enter 7 for CSRT')\n",
    "    \n",
    "    choise = input('Enter your tracker ')\n",
    "    \n",
    "    if choise == '0':\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    elif choise == '1':\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    elif choise == '2':\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    elif choise == '3':\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    elif choise == '4':\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "    elif choise == '5':\n",
    "        tracker = cv2.TrackerGOTURN_create()\n",
    "    elif choise == '6':\n",
    "        tracker = cv2.TrackerMOSSE_create()\n",
    "    elif choise == '7':\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "        \n",
    "    return tracker\n",
    "\n",
    "tracker = ask_for_tracker()            # Tracker\n",
    "tracker_name = str(tracker).split()[0][1:]  # Tracker name\n",
    "cap = cv2.VideoCapture('./Videos/Vehicles.mp4')\n",
    "\n",
    "ret, frame = cap.read()\n",
    "roi = cv2.selectROI(frame, False)\n",
    "ret = tracker.init(frame, roi)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    success, roi = tracker.update(frame)  # update tracker\n",
    "    (x, y, w, h) = tuple(map(int, roi))   # roi -> from tuple to int\n",
    "    cv2.putText(frame, str(tracker), (100,120), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0),2,cv2.LINE_AA)\n",
    "    \n",
    "    if success:                           # Sucess on tracking\n",
    "        pts1 = (x,y)\n",
    "        pts2 = (x+w, y+h)\n",
    "        cv2.rectangle(frame, pts1, pts2, (255, 125, 25), 3)  # Draw rects as tracker moves\n",
    "        \n",
    "    else:                      # Failure on tracking  \n",
    "        cv2.putText(frame, 'Fail to trace the object',(100, 100),cv2.FONT_HERSHEY_SIMPLEX, 1, (25, 125, 255),3)\n",
    "    \n",
    "    cv2.putText(frame,tracker_name,(20, 400),cv2.FONT_HERSHEY_SIMPLEX, 1, (225, 225, 0), 3)  # Display Tracker\n",
    "    cv2.imshow(tracker_name, frame)\n",
    "\n",
    "    if cv2.waitKey(50) & 0xFF == 27:\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ▣ Multi Object Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tracking algorithm 'KCF' \n",
      "Available algorithms are: \n",
      "\n",
      "BOOSTING\n",
      "MIL\n",
      "KCF\n",
      "TLD\n",
      "MEADIANFLOW\n",
      "GOTURN\n",
      "MOSSE\n",
      "CSRT\n",
      "Press q to stop selecting boxes and start multitracking\n",
      "Press any key to select another box\n",
      "Selected boxes [(787, 389, 79, 75)]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "from random import randint\n",
    "tracker_types = ['BOOSTING',\n",
    "                'MIL',\n",
    "                'KCF',\n",
    "                'TLD',\n",
    "                'MEADIANFLOW',\n",
    "                'GOTURN',\n",
    "                'MOSSE',\n",
    "                'CSRT']\n",
    "\n",
    "def tracker_name(tracker_type):\n",
    "\n",
    "    if tracker_type == tracker_types[0]:\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    elif tracker_type == tracker_types[1]:\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    elif tracker_type == tracker_types[2]:\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    elif tracker_type == tracker_types[3]:\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    elif tracker_type == tracker_types[4]:\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "    elif tracker_type == tracker_types[5]:\n",
    "        tracker = cv2.TrackerGOTURN_create()\n",
    "    elif tracker_type == tracker_types[6]:\n",
    "        tracker = cv2.TrackerMOSSE_create()\n",
    "    elif tracker_type == tracker_types[7]:\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "    else:\n",
    "        tracker = None\n",
    "        print('No tracker found')\n",
    "        print('Choose from this trackers: ')\n",
    "        for tr in tracker_types:\n",
    "            print(tr)\n",
    "                                    \n",
    "    return tracker        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Default tracking algorithm 'KCF' \\n\" \"Available algorithms are: \\n\")\n",
    "    for ta in tracker_types:\n",
    "        print(ta)\n",
    "    tracker_type = 'KCF'\n",
    "    cap = cv2.VideoCapture('./Videos/Vehicles.mp4')\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print('Cannot read the video')\n",
    "    rects = []\n",
    "    colors = []\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        rect_box = cv2.selectROI('Multitracker', frame)\n",
    "        rects.append(rect_box)\n",
    "        colors.append((randint(64, 255), randint(64,255), randint(64,255)))\n",
    "        print('Press q to stop selecting boxes and start multitracking')\n",
    "        print('Press any key to select another box')\n",
    "\n",
    "        if cv2.waitKey(0) & 0xFF == 27:\n",
    "            break\n",
    "    print(f'Selected boxes {rects}')\n",
    "    multitracker = cv2.MultiTracker_create()  # Create multitracker\n",
    "    \n",
    "    for rect_box in rects:                    # Initialize multitracker\n",
    "        multitracker.add(tracker_name(tracker_type),frame,rect_box)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        success, boxes = multitracker.update(frame)  # update location objects\n",
    "        \n",
    "        for i, newbox in enumerate(boxes):    # draw the objectes tracked\n",
    "            pts1 = (int(newbox[0]), int(newbox[1]))\n",
    "            pts2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "            cv2.rectangle(frame, pts1, pts2, colors[i], 2, 1)\n",
    "        \n",
    "        cv2.imshow('Multitracker', frame)\n",
    "        \n",
    "        if cv2.waitKey(20) & 0xFF == 27:\n",
    "            break    \n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV-py36",
   "language": "python",
   "name": "cv_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
