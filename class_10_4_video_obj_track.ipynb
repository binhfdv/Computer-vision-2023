{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ðŸ‘‰ class_10_4 Video Â» _Object Tracking_ </center>\n",
    "\n",
    "## Object Tracking \n",
    "\n",
    "Simply put, locating an object in successive frames of a video is called tracking.  \n",
    "The definition sounds straight forward but in computer vision and machine learning, tracking is a very broad term that encompasses conceptually similar but technically different ideas.   \n",
    "For example, all the following different but related ideas are generally studied under __Object Tracking__  \n",
    "\n",
    "        ê°„ë‹¨ížˆ ë§í•´ì„œ ë¹„ë””ì˜¤ì˜ ì—°ì† í”„ë ˆìž„ì—ì„œ ê°œì²´ë¥¼ ì°¾ëŠ” ê²ƒì„ ì¶”ì ì´ë¼ê³  í•©ë‹ˆë‹¤.  \n",
    "        ì •ì˜ëŠ” ê°„ë‹¨í•´ ë³´ì´ì§€ë§Œ ì»´í“¨í„° ë¹„ì „ê³¼ ê¸°ê³„ í•™ìŠµì—ì„œ ì¶”ì ì€ ê°œë…ì ìœ¼ë¡œ ìœ ì‚¬í•˜ì§€ë§Œ ê¸°ìˆ ì ìœ¼ë¡œ ë‹¤ë¥¸ ì•„ì´ë””ì–´ë¥¼ í¬ê´„í•˜ëŠ” ë§¤ìš° ê´‘ë²”ìœ„í•œ ìš©ì–´ìž…ë‹ˆë‹¤.   \n",
    "        ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì§€ë§Œ ê´€ë ¨ëœ ì•„ì´ë””ì–´ëŠ” ëª¨ë‘ ì¼ë°˜ì ìœ¼ë¡œ ê°ì²´ ì¶”ì ì—ì„œ ì—°êµ¬ë©ë‹ˆë‹¤\n",
    "        \n",
    "* **Dense Optical flow:** These algorithms help estimate the motion vector of every pixel in a video frame.\n",
    "* **Sparse optical flow:** These algorithms, like the Kanade-Lucas-Tomashi (KLT) feature tracker, track the location of a few feature points in an image.\n",
    "* **Kalman Filtering:** A very popular signal processing algorithm used to predict the location of a moving object based on prior motion information.   \n",
    ">- One of the early applications of this algorithm was missile guidance!   \n",
    ">- Also \"the on-board computer that guided the descent of the Apollo 11 lunar module to the moon had a Kalman filter\".  \n",
    "* **Meanshift and Camshift:** These are algorithms for locating the maxima of a density function. They are also used for tracking.\n",
    "* **Single object trackers:** In this class of trackers, the first frame is marked using a rectangle to indicate the location of the object we want to track.   \n",
    "The object is then tracked in subsequent frames using the tracking algorithm.   \n",
    "In most real-life applications, these trackers are used in conjunction with an object detector.\n",
    "* **Multiple object track finding algorithms:** In cases when we have a fast object detector, it makes sense to detect multiple objects in each frame and then run a track finding algorithm that identifies which rectangle in one frame corresponds to a rectangle in the next frame.\n",
    ">- Multiple Object Tracking has come a long way.   \n",
    ">- It uses object detection and novel motion prediction algorithms to get accurate tracking information.   \n",
    ">- For example, DeepSort uses the YOLO network to get blazing-fast inference speed. It is based on SORT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object tracking using OpenCV 4 â€“ the Tracking API  \n",
    "\n",
    "OpenCV 4 comes with a tracking API that contains implementations of many single object tracking algorithms.   \n",
    "There are 8 different trackers available in OpenCV 4.2:\n",
    "* **BOOSTING, MIL, KCF, TLD, MEDIANFLOW, GOTURN, MOSSE, and CSRT.**   \n",
    "  \n",
    "- OpenCV 3.2 has implementations of these 6 trackers â€” BOOSTING, MIL, TLD, MEDIANFLOW, MOSSE, and GOTURN.   \n",
    "- OpenCV 3.1 has implementations of these 5 trackers â€” BOOSTING, MIL, KCF, TLD, MEDIANFLOW.   \n",
    "- OpenCV 3.0 has implementations of the following 4 trackers â€” BOOSTING, MIL, TLD, MEDIANFLOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object tracking using OpenCV â€“ the Algorithms  \n",
    "\n",
    "Let's begin by first explaining some general principles behind tracking.   \n",
    "- In tracking, our goal is to find an object in the current frame given we have tracked the object successfully in all (or nearly all) previous frames.  \n",
    "- Since we have tracked the object up until the current frame, we know how it has been moving.   \n",
    "- In other words, we know the parameters of the motion model.   \n",
    ">- The motion model is just a fancy way of saying that you know the location and the velocity (speed + direction of motion) of the object in previous frames.   \n",
    ">- If you knew nothing else about the object, you could predict the new location based on the current motion model, and you would be pretty close to where the new location of the object is.  \n",
    ">- But we have more information than just the motion of the object.   \n",
    "- We know how the object looks in each of the previous frames.   \n",
    "- In other words, we can build an appearance model that encodes what the object looks like.   \n",
    "- This appearance model can be used to search in a small neighborhood of the location predicted by the motion model to more accurately predict the location of the object.  \n",
    "- The motion model predicts the approximate location of the object.   \n",
    "- The appearance model fine tunes this estimate to provide a more accurate estimate based on appearance.  \n",
    "- If the object was very simple and did not change itâ€™s appearance much, we could use a simple template as an appearance model and look for that template.   \n",
    "- However, real life is not that simple. The appearance of an object can change dramatically.   \n",
    "- To tackle this problem, in many modern trackers, this appearance model is a classifier that is trained in an online manner.   \n",
    "\n",
    "        ë¨¼ì € ì¶”ì ì˜ ëª‡ ê°€ì§€ ì¼ë°˜ì ì¸ ì›ì¹™ì„ ì„¤ëª…í•˜ëŠ” ê²ƒë¶€í„° ì‹œìž‘í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "        - ì¶”ì ì—ì„œ ìš°ë¦¬ì˜ ëª©í‘œëŠ” ëª¨ë“ (ë˜ëŠ” ê±°ì˜ ëª¨ë“ ) ì´ì „ í”„ë ˆìž„ì—ì„œ ê°ì²´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì í•œ ê²½ìš° í˜„ìž¬ í”„ë ˆìž„ì—ì„œ ê°ì²´ë¥¼ ì°¾ëŠ” ê²ƒìž…ë‹ˆë‹¤.\n",
    "        - í˜„ìž¬ í”„ë ˆìž„ê¹Œì§€ ë¬¼ì²´ë¥¼ ì¶”ì í–ˆê¸° ë•Œë¬¸ì— ë¬¼ì²´ê°€ ì–´ë–»ê²Œ ì›€ì§ì´ê³  ìžˆëŠ”ì§€ ì•Œ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ì¦‰, ëª¨ì…˜ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì•Œê³  ìžˆìŠµë‹ˆë‹¤.\n",
    "        >- ëª¨ì…˜ ëª¨ë¸ì€ ì´ì „ í”„ë ˆìž„ì—ì„œ ê°ì²´ì˜ ìœ„ì¹˜ì™€ ì†ë„(ì†ë„ + ëª¨ì…˜ ë°©í–¥)ë¥¼ ì•Œê³  ìžˆë‹¤ëŠ” ê²ƒì„ í‘œí˜„í•˜ëŠ” ë©‹ì§„ ë°©ë²•ìž…ë‹ˆë‹¤.\n",
    "        >- ë¬¼ì²´ì— ëŒ€í•´ ì•„ë¬´ê²ƒë„ ëª¨ë¥¸ë‹¤ë©´ í˜„ìž¬ ëª¨ì…˜ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ìœ„ì¹˜ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìžˆìœ¼ë©° ë¬¼ì²´ì˜ ìƒˆ ìœ„ì¹˜ì— ê½¤ ê°€ê¹ìŠµë‹ˆë‹¤.\n",
    "        >- í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ë¬¼ì²´ì˜ ì›€ì§ìž„ë³´ë‹¤ ë” ë§Žì€ ì •ë³´ë¥¼ ê°–ê³  ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ìš°ë¦¬ëŠ” ì´ì „ í”„ë ˆìž„ ê°ê°ì—ì„œ ê°ì²´ê°€ ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ ì•Œê³  ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ì¦‰, ê°ì²´ì˜ ëª¨ìŠµì„ ì¸ì½”ë”©í•˜ëŠ” ì™¸ê´€ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ì´ ì™¸ê´€ ëª¨ë¸ì€ ëª¨ì…˜ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ìœ„ì¹˜ì˜ ìž‘ì€ ì£¼ë³€ ì§€ì—­ì„ ê²€ìƒ‰í•˜ì—¬ ë¬¼ì²´ì˜ ìœ„ì¹˜ë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ëª¨ì…˜ ëª¨ë¸ì€ ë¬¼ì²´ì˜ ëŒ€ëžµì ì¸ ìœ„ì¹˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "        - ëª¨ì–‘ ëª¨ë¸ì€ ì´ ì¶”ì •ì¹˜ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ëª¨ì–‘ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ë‹¤ ì •í™•í•œ ì¶”ì •ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "        - ê°ì²´ê°€ ë§¤ìš° ë‹¨ìˆœí•˜ê³  ì™¸ê´€ì´ ë§Žì´ ë³€í•˜ì§€ ì•Šì€ ê²½ìš° ê°„ë‹¨í•œ í…œí”Œë¦¿ì„ ì™¸ê´€ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ê³  í•´ë‹¹ í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - í•˜ì§€ë§Œ ì‹¤ì œ ìƒí™œì€ ê·¸ë ‡ê²Œ ê°„ë‹¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬¼ì²´ì˜ ëª¨ì–‘ì€ ê·¹ì ìœ¼ë¡œ ë°”ë€” ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Žì€ ìµœì‹  ì¶”ì ê¸°ì—ì„œ ì´ ëª¨ì–‘ ëª¨ë¸ì€ ì˜¨ë¼ì¸ ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ëœ ë¶„ë¥˜ìžìž…ë‹ˆë‹¤.  \n",
    "        \n",
    "The job of the classifier is to classify a rectangular region of an image as either an object or background.   \n",
    "The classifier takes in an image patch as input and returns a score between 0 and 1 to indicate the probability that the image patch contains the object.   \n",
    "The score is 0 when it is absolutely sure the image patch is the background and 1 when it is absolutely sure the patch is the object.  \n",
    "\n",
    "In machine learning, we use the word â€œonlineâ€ to refer to algorithms that are trained on the fly at run time.   \n",
    "An offline classifier may need thousands of examples to train a classifier, but an online classifier is typically trained using very few examples at run time.  \n",
    "\n",
    "A classifier is trained by feeding it positive (object) and negative (background) examples.   \n",
    "If you want to build a classifier for detecting cats, you train it with thousands of images containing cats and thousands of images that do not contain cats.   \n",
    "This way the classifier learns to differentiate what is a cat and what is not.   \n",
    "While building an online classifier, we do not have the luxury of having thousands of examples of the positive and negative classes.  \n",
    "\n",
    "        ë¶„ë¥˜ìžì˜ ì—­í• ì€ ì´ë¯¸ì§€ì˜ ì§ì‚¬ê°í˜• ì˜ì—­ì„ ê°ì²´ ë˜ëŠ” ë°°ê²½ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤.\n",
    "        ë¶„ë¥˜ê¸°ëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ìž…ë ¥ìœ¼ë¡œ ì·¨í•˜ê³  0ì—ì„œ 1 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ì—¬ ì´ë¯¸ì§€ íŒ¨ì¹˜ì— ê°ì²´ê°€ í¬í•¨ë  í™•ë¥ ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "        ì´ë¯¸ì§€ íŒ¨ì¹˜ê°€ ë°°ê²½ì´ë¼ê³  ì ˆëŒ€ì ìœ¼ë¡œ í™•ì‹ í•  ë•Œ ì ìˆ˜ëŠ” 0ì´ê³ , íŒ¨ì¹˜ê°€ ê°ì²´ë¼ê³  ì ˆëŒ€ì ìœ¼ë¡œ í™•ì‹ í•  ë•Œ ì ìˆ˜ëŠ” 1ìž…ë‹ˆë‹¤.\n",
    "\n",
    "        ê¸°ê³„ í•™ìŠµì—ì„œëŠ” \"ì˜¨ë¼ì¸\"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŸ°íƒ€ìž„ì— ì¦‰ì„ì—ì„œ í›ˆë ¨ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "        ì˜¤í”„ë¼ì¸ ë¶„ë¥˜ìžëŠ” ë¶„ë¥˜ìžë¥¼ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ ìˆ˜ì²œ ê°œì˜ ì˜ˆê°€ í•„ìš”í•  ìˆ˜ ìžˆì§€ë§Œ ì˜¨ë¼ì¸ ë¶„ë¥˜ìžëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëŸ°íƒ€ìž„ì— ë§¤ìš° ì ì€ ìˆ˜ì˜ ì˜ˆë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "        ë¶„ë¥˜ìžëŠ” ê¸ì •ì (ê°ì²´) ë° ë¶€ì •ì (ë°°ê²½) ì˜ˆì‹œë¥¼ ì œê³µí•˜ì—¬ í›ˆë ¨ë©ë‹ˆë‹¤.\n",
    "        ê³ ì–‘ì´ë¥¼ ê°ì§€í•˜ê¸° ìœ„í•œ ë¶„ë¥˜ê¸°ë¥¼ êµ¬ì¶•í•˜ë ¤ë©´ ê³ ì–‘ì´ê°€ í¬í•¨ëœ ìˆ˜ì²œ ê°œì˜ ì´ë¯¸ì§€ì™€ ê³ ì–‘ì´ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ìˆ˜ì²œ ê°œì˜ ì´ë¯¸ì§€ë¡œ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜ìžëŠ” ê³ ì–‘ì´ì¸ì§€ ì•„ë‹Œì§€ë¥¼ êµ¬ë³„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "        ì˜¨ë¼ì¸ ë¶„ë¥˜ê¸°ë¥¼ êµ¬ì¶•í•˜ëŠ” ë™ì•ˆ ìš°ë¦¬ëŠ” ê¸ì •ì ì¸ í´ëž˜ìŠ¤ì™€ ë¶€ì •ì ì¸ í´ëž˜ìŠ¤ì˜ ìˆ˜ì²œ ê°€ì§€ ì˜ˆë¥¼ ê°€ì§ˆ ì—¬ìœ ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### â— BOOSTING Tracker  \n",
    "\n",
    "This tracker is based on an online version of AdaBoost â€” the algorithm that the HAAR cascade based face detector uses internally. This classifier needs to be trained at runtime with positive and negative examples of the object. The initial bounding box supplied by the user ( or by another object detection algorithm ) is taken as a positive example for the object, and many image patches outside the bounding box are treated as the background.  \n",
    "Given a new frame, the classifier is run on every pixel in the neighborhood of the previous location and the score of the classifier is recorded. The new location of the object is the one where the score is maximum. So now we have one more positive example for the classifier.   \n",
    "As more frames come in, the classifier is updated with this additional data.\n",
    "- Pros: None. This algorithm is a decade old and works ok, but I could not find a good reason to use it especially when other advanced trackers (MIL, KCF) based on similar principles are available.  \n",
    "- Cons: Tracking performance is mediocre. It does not reliably know when tracking has failed.  \n",
    "    \n",
    "        ì´ ì¶”ì ê¸°ëŠ” HAAR ìºìŠ¤ì¼€ì´ë“œ ê¸°ë°˜ ì–¼êµ´ ê°ì§€ê¸°ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì¸ AdaBoostì˜ ì˜¨ë¼ì¸ ë²„ì „ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì´ ë¶„ë¥˜ìžëŠ” ê°ì²´ì˜ ê¸ì •ì ì¸ ì˜ˆì™€ ë¶€ì •ì ì¸ ì˜ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ëŸ°íƒ€ìž„ì— í›ˆë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ìš©ìž(ë˜ëŠ” ë‹¤ë¥¸ ê°ì²´ ê°ì§€ ì•Œê³ ë¦¬ì¦˜)ê°€ ì œê³µí•œ ì´ˆê¸° ê²½ê³„ ìƒìžëŠ” ê°ì²´ì— ëŒ€í•œ ê¸ì •ì ì¸ ì˜ˆë¡œ ê°„ì£¼ë˜ë©° ê²½ê³„ ìƒìž ì™¸ë¶€ì˜ ë§Žì€ ì´ë¯¸ì§€ íŒ¨ì¹˜ëŠ” ë°°ê²½ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n",
    "        ìƒˆë¡œìš´ í”„ë ˆìž„ì´ ì£¼ì–´ì§€ë©´ ë¶„ë¥˜ê¸°ëŠ” ì´ì „ ìœ„ì¹˜ ê·¼ì²˜ì˜ ëª¨ë“  í”½ì…€ì—ì„œ ì‹¤í–‰ë˜ê³  ë¶„ë¥˜ê¸°ì˜ ì ìˆ˜ê°€ ê¸°ë¡ë©ë‹ˆë‹¤. ê°œì²´ì˜ ìƒˆ ìœ„ì¹˜ëŠ” ì ìˆ˜ê°€ ìµœëŒ€ì¸ ìœ„ì¹˜ìž…ë‹ˆë‹¤. ì´ì œ ë¶„ë¥˜ê¸°ì— ëŒ€í•œ ê¸ì •ì ì¸ ì˜ˆê°€ í•˜ë‚˜ ë” ìžˆìŠµë‹ˆë‹¤.\n",
    "        ë” ë§Žì€ í”„ë ˆìž„ì´ ë“¤ì–´ì˜¤ë©´ ë¶„ë¥˜ìžëŠ” ì´ ì¶”ê°€ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.\n",
    "        - ìž¥ì : ì—†ìŒ. ì´ ì•Œê³ ë¦¬ì¦˜ì€ 10ë…„ì´ ì§€ë‚¬ê³  ìž˜ ìž‘ë™í•˜ì§€ë§Œ íŠ¹ížˆ ìœ ì‚¬í•œ ì›ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë‹¤ë¥¸ ê³ ê¸‰ ì¶”ì ê¸°(MIL, KCF)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ê²½ìš° ì´ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì•¼ í•  íƒ€ë‹¹í•œ ì´ìœ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "        - ë‹¨ì : ì¶”ì  ì„±ëŠ¥ì´ í‰ë²”í•©ë‹ˆë‹¤. ì¶”ì ì´ ì–¸ì œ ì‹¤íŒ¨í–ˆëŠ”ì§€ í™•ì‹¤í•˜ê²Œ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "            \n",
    "### â— MIL Tracker  \n",
    "This tracker is similar in idea to the BOOSTING tracker described above.   \n",
    "The big difference is that instead of considering only the current location of the object as a positive example, it looks in a small neighborhood around the current location to generate several potential positive examples.   \n",
    "You may be thinking that it is a bad idea because in most of these â€œpositiveâ€ examples the object is not centered.\n",
    "This is where Multiple Instance Learning (MIL) comes to rescue. In MIL, you do not specify positive and negative examples, but positive and negative â€œbagsâ€.  \n",
    "The collection of images in the positive bag are not all positive examples. Instead, only one image in the positive bag needs to be a positive example!  \n",
    "In our example, a positive bag contains the patch centered on the current location of the object and also patches in a small neighborhood around it.   \n",
    "Even if the current location of the tracked object is not accurate, when samples from the neighborhood of the current location are put in the positive bag, there is a good chance that this bag contains at least one image in which the object is nicely centered.   \n",
    "MIL project page has more information for people who like to dig deeper into the inner workings of the MIL tracker.\n",
    "- Pros: The performance is pretty good. It does not drift as much as the BOOSTING tracker and it does a reasonable job under partial occlusion. If you are using OpenCV 3.0, this might be the best tracker available to you. But if you are using a higher version, consider KCF.\n",
    "- Cons: Tracking failure is not reported reliably. Does not recover from full occlusion.  \n",
    "    \n",
    "        ì´ ì¶”ì ê¸°ëŠ” ìœ„ì—ì„œ ì„¤ëª…í•œ BOOSTING ì¶”ì ê¸°ì™€ ì•„ì´ë””ì–´ê°€ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
    "        ê°€ìž¥ í° ì°¨ì´ì ì€ ê°ì²´ì˜ í˜„ìž¬ ìœ„ì¹˜ë§Œì„ ê¸ì •ì ì¸ ì˜ˆë¡œ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í˜„ìž¬ ìœ„ì¹˜ ì£¼ë³€ì˜ ìž‘ì€ ë™ë„¤ë¥¼ ì‚´íŽ´ë³´ë©° ì—¬ëŸ¬ ê°€ì§€ ìž ìž¬ì ì¸ ê¸ì •ì ì¸ ì‚¬ë¡€ë¥¼ ìƒì„±í•œë‹¤ëŠ” ì ìž…ë‹ˆë‹¤.\n",
    "        ëŒ€ë¶€ë¶„ì˜ \"ê¸ì •ì ì¸\" ì˜ˆì—ì„œ ê°ì²´ê°€ ì¤‘ì•™ì— ìžˆì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ê²ƒì´ ë‚˜ìœ ìƒê°ì´ë¼ê³  ìƒê°í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.\n",
    "        ì´ê²ƒì´ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ í•™ìŠµ(MIL)ì´ êµ¬ì¶œë˜ëŠ” ê³³ìž…ë‹ˆë‹¤. MILì—ì„œëŠ” ê¸ì •ì ì¸ ì˜ˆì™€ ë¶€ì •ì ì¸ ì˜ˆë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê¸ì •ì ì¸ ì˜ˆì™€ ë¶€ì •ì ì¸ \"ë°±\"ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "        í¬ì§€í‹°ë¸Œ ë°±ì— ë‹´ê¸´ ì´ë¯¸ì§€ ëª¨ìŒì´ ëª¨ë‘ í¬ì§€í‹°ë¸Œ ì˜ˆì‹œëŠ” ì•„ë‹™ë‹ˆë‹¤. ëŒ€ì‹ , í¬ì§€í‹°ë¸Œ ë°±ì— ìžˆëŠ” ì´ë¯¸ì§€ í•˜ë‚˜ë§Œ í¬ì§€í‹°ë¸Œ ì˜ˆì‹œê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤!\n",
    "        ì´ ì˜ˆì—ì„œ í¬ì§€í‹°ë¸Œ ë°±ì—ëŠ” ê°ì²´ì˜ í˜„ìž¬ ìœ„ì¹˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ëŠ” íŒ¨ì¹˜ì™€ ê°ì²´ ì£¼ë³€ì˜ ìž‘ì€ ì´ì›ƒì— ìžˆëŠ” íŒ¨ì¹˜ë„ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.\n",
    "        ì¶”ì ëœ ê°ì²´ì˜ í˜„ìž¬ ìœ„ì¹˜ê°€ ì •í™•í•˜ì§€ ì•Šë”ë¼ë„ í˜„ìž¬ ìœ„ì¹˜ ê·¼ì²˜ì˜ ìƒ˜í”Œì„ í¬ì§€í‹°ë¸Œ ë°±ì— ë„£ìœ¼ë©´ ì´ ë°±ì— ê°ì²´ê°€ ì¤‘ì•™ì— ìž˜ ë°°ì¹˜ëœ ì´ë¯¸ì§€ê°€ í•˜ë‚˜ ì´ìƒ í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "        MIL í”„ë¡œì íŠ¸ íŽ˜ì´ì§€ì—ëŠ” MIL ì¶”ì ê¸°ì˜ ë‚´ë¶€ ìž‘ë™ ë°©ì‹ì„ ë” ê¹Šì´ íŒŒê³ ë“œëŠ” ì‚¬ëžŒë“¤ì„ ìœ„í•œ ë” ë§Žì€ ì •ë³´ê°€ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ìž¥ì  : ì„±ëŠ¥ì´ ê½¤ ì¢‹ìŠµë‹ˆë‹¤. BOOSTING ì¶”ì ê¸°ë§Œí¼ ë“œë¦¬í”„íŠ¸í•˜ì§€ ì•Šìœ¼ë©° ë¶€ë¶„ íì‡„ í•˜ì—ì„œ í•©ë¦¬ì ì¸ ìž‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. OpenCV 3.0ì„ ì‚¬ìš©í•˜ê³  ìžˆë‹¤ë©´ ì´ê²ƒì´ ìµœê³ ì˜ íŠ¸ëž˜ì»¤ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë” ë†’ì€ ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° KCFë¥¼ ê³ ë ¤í•˜ì‹­ì‹œì˜¤.\n",
    "        - ë‹¨ì : ì¶”ì  ì‹¤íŒ¨ê°€ ì•ˆì •ì ìœ¼ë¡œ ë³´ê³ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì™„ì „ íì‡„ì—ì„œ íšŒë³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
    "    \n",
    "### â— KCF Tracker\n",
    "KFC stands for Kernelized Correlation Filters. This tracker builds on the ideas presented in the previous two trackers. This tracker utilizes the fact that the multiple positive samples used in the MIL tracker have large overlapping regions. This overlapping data leads to some nice mathematical properties that are exploited by this tracker to make tracking faster and more accurate at the same time.\n",
    "- Pros: Accuracy and speed are both better than MIL and it reports tracking failure better than BOOSTING and MIL.   \n",
    "If you are using OpenCV 3.1 and above, I recommend using this for most applications.\n",
    "- Cons: Does not recover from full occlusion. \n",
    "    \n",
    "        KFCëŠ” ì»¤ë„í™”ëœ ìƒê´€ í•„í„°(Kernelized Correlation Filter)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ì¶”ì ê¸°ëŠ” ì´ì „ ë‘ ì¶”ì ê¸°ì—ì„œ ì œì‹œëœ ì•„ì´ë””ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì´ ì¶”ì ê¸°ëŠ” MIL ì¶”ì ê¸°ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ì–‘ì„± ìƒ˜í”Œì´ í° ì¤‘ì²© ì˜ì—­ì„ ê°€ì§€ê³  ìžˆë‹¤ëŠ” ì‚¬ì‹¤ì„ í™œìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¤‘ë³µë˜ëŠ” ë°ì´í„°ëŠ” ì´ ì¶”ì ê¸°ê°€ í™œìš©í•˜ì—¬ ë™ì‹œì— ë” ë¹ ë¥´ê³  ì •í™•í•œ ì¶”ì ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ëª‡ ê°€ì§€ ë©‹ì§„ ìˆ˜í•™ì  ì†ì„±ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.\n",
    "        - ìž¥ì : ì •í™•ì„±ê³¼ ì†ë„ ëª¨ë‘ MILë³´ë‹¤ ìš°ìˆ˜í•˜ë©° BOOSTING ë° MILë³´ë‹¤ ì¶”ì  ì‹¤íŒ¨ë¥¼ ë” ìž˜ ë³´ê³ í•©ë‹ˆë‹¤.\n",
    "        OpenCV 3.1 ì´ìƒì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ëŒ€ë¶€ë¶„ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì´ ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "        - ë‹¨ì : ì™„ì „ íì‡„ì—ì„œ íšŒë³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
    "            \n",
    "### â— TLD Tracker\n",
    "TLD stands for Tracking, learning, and detection. As the name suggests, this tracker decomposes the long term tracking task into three components â€” (short term) tracking, learning, and detection. From the authorâ€™s paper, â€œThe tracker follows the object from frame to frame. The detector localizes all appearances that have been observed so far and corrects the tracker if necessary.\n",
    "The learning estimates detectorâ€™s errors and updates it to avoid these errors in the future.â€ This output of this tracker tends to jump around a bit. For example, if you are tracking a pedestrian and there are other pedestrians in the scene, this tracker can sometimes temporarily track a different pedestrian than the one you intended to track. On the positive side, this track appears to track an object over a larger scale, motion, and occlusion. If you have a video sequence where the object is hidden behind another object, this tracker may be a good choice.\n",
    "- Pros: Works the best under occlusion over multiple frames. Also, tracks best over scale changes.\n",
    "- Cons: Lots of false positives making it almost unusable.  \n",
    "    \n",
    "TLDëŠ” ì¶”ì (Tracking), í•™ìŠµ(Learning), íƒì§€(Detection)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìžˆë“¯ì´ ì´ ì¶”ì ê¸°ëŠ” ìž¥ê¸° ì¶”ì  ìž‘ì—…ì„ (ë‹¨ê¸°) ì¶”ì , í•™ìŠµ ë° íƒì§€ì˜ ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¡œ ë¶„í•´í•©ë‹ˆë‹¤. ì €ìžì˜ ë…¼ë¬¸ì—ì„œ â€œì¶”ì ê¸°ëŠ” í”„ë ˆìž„ë§ˆë‹¤ ê°œì²´ë¥¼ ë”°ë¼ê°‘ë‹ˆë‹¤. ê²€ì¶œê¸°ëŠ” ì§€ê¸ˆê¹Œì§€ ê´€ì°°ëœ ëª¨ë“  ëª¨ì–‘ì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ê³  í•„ìš”í•œ ê²½ìš° ì¶”ì ê¸°ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
    "í•™ìŠµì€ ê°ì§€ê¸°ì˜ ì˜¤ë¥˜ë¥¼ ì¶”ì •í•˜ê³  ì´ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ í–¥í›„ ì´ëŸ¬í•œ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\" ì´ ì¶”ì ê¸°ì˜ ì¶œë ¥ì€ ì•½ê°„ ì í”„í•˜ëŠ” ê²½í–¥ì´ ìžˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë³´í–‰ìžë¥¼ ì¶”ì í•˜ê³  ìžˆëŠ”ë° ìž¥ë©´ì— ë‹¤ë¥¸ ë³´í–‰ìžê°€ ìžˆëŠ” ê²½ìš° ì´ ì¶”ì ê¸°ëŠ” ë•Œë•Œë¡œ ì¶”ì í•˜ë ¤ëŠ” ë³´í–‰ìžê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë³´í–‰ìžë¥¼ ì¼ì‹œì ìœ¼ë¡œ ì¶”ì í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ê¸ì •ì ì¸ ì¸¡ë©´ì—ì„œ ì´ íŠ¸ëž™ì€ ë” í° ê·œëª¨, ëª¨ì…˜ ë° íìƒ‰ì— ê±¸ì³ ê°œì²´ë¥¼ ì¶”ì í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ìž…ë‹ˆë‹¤. ê°œì²´ê°€ ë‹¤ë¥¸ ê°œì²´ ë’¤ì— ìˆ¨ê²¨ì ¸ ìžˆëŠ” ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ê°€ ìžˆëŠ” ê²½ìš° ì´ ì¶”ì ê¸°ê°€ ì¢‹ì€ ì„ íƒì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "- ìž¥ì : ì—¬ëŸ¬ í”„ë ˆìž„ì— ê±¸ì³ íìƒ‰ëœ ìƒíƒœì—ì„œ ê°€ìž¥ ìž˜ ìž‘ë™í•©ë‹ˆë‹¤. ë˜í•œ ê·œëª¨ ë³€í™”ì— ëŒ€í•´ ê°€ìž¥ ìž˜ ì¶”ì í•©ë‹ˆë‹¤.\n",
    "- ë‹¨ì : ì˜¤íƒ(false positive)ì´ ë§Žì•„ ê±°ì˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.  \n",
    "    \n",
    "### â— MEDIANFLOW Tracker\n",
    "Internally, this tracker tracks the object in both forward and backward directions in time and measures the discrepancies between these two trajectories.   \n",
    "Minimizing this ForwardBackward error enables them to reliably detect tracking failures and select reliable trajectories in video sequences.\n",
    "In my tests, I found this tracker works best when the motion is predictable and small.   \n",
    "Unlike, other trackers that keep going even when the tracking has clearly failed, this tracker knows when the tracking has failed.\n",
    "- Pros: Excellent tracking failure reporting. Works very well when the motion is predictable and there is no occlusion.  \n",
    "- Cons: Fails under large motion.  \n",
    "    \n",
    "        ë‚´ë¶€ì ìœ¼ë¡œ ì´ ì¶”ì ê¸°ëŠ” ì‹œê°„ì— ë”°ë¼ ì•žë’¤ ë°©í–¥ìœ¼ë¡œ ë¬¼ì²´ë¥¼ ì¶”ì í•˜ê³  ì´ ë‘ ê¶¤ì  ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "        ì´ ForwardBackward ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ë©´ ì¶”ì  ì‹¤íŒ¨ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ê°ì§€í•˜ê³  ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” ê¶¤ì ì„ ì„ íƒí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        í…ŒìŠ¤íŠ¸ì—ì„œ ì´ ì¶”ì ê¸°ëŠ” ì›€ì§ìž„ì´ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ìž‘ì„ ë•Œ ê°€ìž¥ ìž˜ ìž‘ë™í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.\n",
    "        ì¶”ì ì´ í™•ì‹¤ížˆ ì‹¤íŒ¨í–ˆìŒì—ë„ ê³„ì† ìž‘ë™í•˜ëŠ” ë‹¤ë¥¸ ì¶”ì ê¸°ì™€ ë‹¬ë¦¬ ì´ ì¶”ì ê¸°ëŠ” ì¶”ì ì´ ì‹¤íŒ¨í•œ ì‹œì ì„ ì•Œ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - ìž¥ì : ì¶”ì  ì‹¤íŒ¨ ë³´ê³ ê°€ ë›°ì–´ë‚©ë‹ˆë‹¤. ëª¨ì…˜ì´ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  íìƒ‰ì´ ì—†ì„ ë•Œ ë§¤ìš° ìž˜ ìž‘ë™í•©ë‹ˆë‹¤.\n",
    "        - ë‹¨ì : í° ì›€ì§ìž„ì—ì„œëŠ” ì‹¤íŒ¨í•©ë‹ˆë‹¤.  \n",
    "            \n",
    "### â— GOTURN tracker\n",
    "Out of all the tracking algorithms in the tracker class, this is the only one based on Convolutional Neural Network (CNN).   \n",
    "From OpenCV documentation, we know it is â€œrobust to viewpoint changes, lighting changes, and deformationsâ€.   \n",
    "But it does not handle occlusion very well.\n",
    "* GOTURN being a CNN based tracker, uses a Caffe model for tracking.   \n",
    "* The Caffe model and the proto text file must be present in the directory in which the code is present.   \n",
    "* These files can also be downloaded from the opencv_extra repository, concatenated, and extracted before use.\n",
    "* Update: GOTURN object tracking algorithm has been ported to OpenCV.\n",
    "\n",
    "### â— MOSSE tracker\n",
    "- Minimum Output Sum of Squared Error (MOSSE) uses an adaptive correlation for object tracking which produces stable correlation filters when initialized using a single frame.   \n",
    "- MOSSE tracker is robust to variations in lighting, scale, pose, and non-rigid deformations.   \n",
    "- It also detects occlusion based upon the peak-to-sidelobe ratio, which enables the tracker to pause and resume where it left off when the object reappears.   \n",
    "- MOSSE tracker also operates at a higher fps (450 fps and even more).   \n",
    "- To add to the positives, it is also very easy to implement, is as accurate as other complex trackers and much faster.   \n",
    "- But, on a performance scale, it lags behind the deep learning based trackers.  \n",
    "\n",
    "        - MOSSE(Minimum Output Sum of Squared Error)ëŠ” ë‹¨ì¼ í”„ë ˆìž„ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”í•  ë•Œ ì•ˆì •ì ì¸ ìƒê´€ í•„í„°ë¥¼ ìƒì„±í•˜ëŠ” ê°œì²´ ì¶”ì ì„ ìœ„í•œ ì ì‘í˜• ìƒê´€ ê´€ê³„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        - MOSSE ì¶”ì ê¸°ëŠ” ì¡°ëª…, ê·œëª¨, í¬ì¦ˆ ë° ë¹„ê°•ì„± ë³€í˜•ì˜ ë³€í™”ì— ê°•ë ¥í•©ë‹ˆë‹¤.\n",
    "        - ë˜í•œ í”¼í¬ ëŒ€ ì‚¬ì´ë“œë¡œë¸Œ ë¹„ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ íìƒ‰ì„ ê°ì§€í•˜ë¯€ë¡œ ê°ì²´ê°€ ë‹¤ì‹œ ë‚˜íƒ€ë‚  ë•Œ ì¶”ì ê¸°ê°€ ì¤‘ë‹¨ëœ ë¶€ë¶„ë¶€í„° ì¼ì‹œ ì¤‘ì§€í•˜ê³  ë‹¤ì‹œ ì‹œìž‘í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        - MOSSE ì¶”ì ê¸°ëŠ” ë” ë†’ì€ fps(450fps ì´ìƒ)ì—ì„œë„ ìž‘ë™í•©ë‹ˆë‹¤.\n",
    "        - ìž¥ì ì„ ì¶”ê°€í•˜ìžë©´, êµ¬í˜„í•˜ê¸°ê°€ ë§¤ìš° ì‰½ê³ , ë‹¤ë¥¸ ë³µìž¡í•œ ì¶”ì ê¸°ë§Œí¼ ì •í™•í•˜ê³  í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.\n",
    "        - í•˜ì§€ë§Œ ì„±ëŠ¥ë©´ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ íŠ¸ëž˜ì»¤ì— ë¹„í•´ ë’¤ë–¨ì–´ì§‘ë‹ˆë‹¤.  \n",
    "        \n",
    "### â— CSRT tracker\n",
    "- In the Discriminative Correlation Filter with Channel and Spatial Reliability (DCF-CSR), we use the spatial reliability map for adjusting the filter support to the part of the selected region from the frame for tracking.   \n",
    "- This ensures enlarging and localization of the selected region and improved tracking of the non-rectangular regions or objects.   \n",
    "- It uses only 2 standard features (HoGs and Colornames).   \n",
    "- It also operates at a comparatively lower fps (25 fps) but gives higher accuracy for object tracking.   \n",
    "\n",
    "        - DCF-CSR(Discriminative Correlation Filter with Channel and Spatial Reliability)ì—ì„œëŠ” ì¶”ì ì„ ìœ„í•´ í”„ë ˆìž„ì—ì„œ ì„ íƒí•œ ì˜ì—­ì˜ ì¼ë¶€ì— ëŒ€í•œ í•„í„° ì§€ì›ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ ê³µê°„ ì‹ ë¢°ë„ ë§µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        - ì´ë¥¼ í†µí•´ ì„ íƒí•œ ì˜ì—­ì˜ í™•ëŒ€ ë° ìœ„ì¹˜í™”ê°€ ë³´ìž¥ë˜ê³  ì§ì‚¬ê°í˜•ì´ ì•„ë‹Œ ì˜ì—­ì´ë‚˜ ê°œì²´ì— ëŒ€í•œ ì¶”ì ì´ í–¥ìƒë©ë‹ˆë‹¤.\n",
    "        - 2ê°€ì§€ í‘œì¤€ ê¸°ëŠ¥(HoG ë° Colornames)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        - ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ fps(25fps)ì—ì„œë„ ìž‘ë™í•˜ì§€ë§Œ ê°ì²´ ì¶”ì ì— ìžˆì–´ ë” ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.      \n",
    "    \n",
    "In the commented code below we first set up the tracker by choosing a tracker type   \n",
    "- BOOSTING, MIL, KCF, TLD, MEDIANFLOW, GOTURN, MOSSE, or CSRT.   \n",
    "We then open a video and grab a frame.   \n",
    "We define a bounding box containing the object for the first frame and initialize the tracker with the first frame and the bounding box.   \n",
    "Finally, we read frames from the video and just update the tracker in a loop to obtain a new bounding box for the current frame.  \n",
    "Results are subsequently displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tracking API  \n",
    "\n",
    "OpenCV provides a Tracking API for tracking objects.   \n",
    "The Tracking API makes it easy to track objects.   \n",
    "The Tracking API constructor provided by OpenCV is shown below.   \n",
    "The constructor varies depending on the algorithm.  \n",
    "\n",
    "        OpenCVì—ì„œëŠ” ê°ì²´ ì¶”ì ì„ ìœ„í•œ Tracking APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.   \n",
    "        Tracking APIë¥¼ ì´ìš©í•˜ë©´ ì‰½ê²Œ ê°ì²´ ì¶”ì ì„ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.   \n",
    "        OpenCVì—ì„œ ì œê³µí•˜ëŠ” Tracking APIìƒì„±ìžëŠ” ì•„ëž˜ì™€ ê°™ìŠµë‹ˆë‹¤.   \n",
    "        ìƒì„±ìžëŠ” ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ë‹¤ì–‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "* **tracker = cv2.TrackerBoosting_create():** Based on the AdaBoost algorithm\n",
    "* **tracker = cv2.TrackerMIL_create():** Based on the MIL(Multiple Instance Learning) algorithm\n",
    "* **tracker = cv2.TrackerKCF_create():** Based on KCF(Kernelized Correlation Filters) algorithm\n",
    "* **tracker = cv2.TrackerTLD_create():** Based on TLD(Tracking, Learning and Detection) algorithm\n",
    "* **tracker = cv2.TrackerMedianFlow_create():** Track the forward and reverse directions of objects to measure inconsistencies ê°ì²´ì˜ ì „ë°©í–¥/ì—­ë°©í–¥ì„ ì¶”ì í•´ì„œ ë¶ˆì¼ì¹˜ì„±ì„ ì¸¡ì •\n",
    "* tracker = cv2.TrackerGOTURN_cretae(): Based on CNN(Convolutional Neural Networks)  not work\n",
    "* tracker = cv2.TrackerCSRT_create(): Based on CSRT(Channel and Spatial Reliability) not work\n",
    "* tracker = cv2.TrackerMOSSE_create(): Use grayscale internally not work\n",
    "\n",
    "- retval = cv2.Tracker.init(img, boundingBox): initialize Tracker \n",
    ">- img: input\n",
    ">- boundingBox: roi (x, y)\n",
    "\n",
    "- retval, boundingBox = cv2.Tracker.update(img): Find the location of the object to be tracked in a new frame\n",
    ">- img: new frame\n",
    ">- retval: track boolean\n",
    ">- boundingBox: new roi (x, y, w, h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you run the code, the screen will freeze.   \n",
    "- First, drag the object you want to track.   \n",
    "- Next, press the Space key. It will then play back on its own, and start tracking the object.   \n",
    "- You can press the keyboard 0~4 number keys to select the tracker algorithm.   \n",
    "- At the top of the screen, you will see 0:TrackerBoosting, 1:TrackerMIL, etc.   \n",
    "- This is an indication of which tracker you are currently using to track an object.\n",
    "\n",
    "ì½”ë“œë¥¼ ì²˜ìŒ ì‹¤í–‰í•˜ë©´ í™”ë©´ì´ ë©ˆì¶°ìžˆì„ ê²ë‹ˆë‹¤.   \n",
    "- ë¨¼ì € ì¶”ì ì„ ì›í•˜ëŠ” ê°ì²´ë¥¼ ë“œëž˜ê·¸í•©ë‹ˆë‹¤.   \n",
    "- ë‹¤ìŒìœ¼ë¡œ ìŠ¤íŽ˜ì´ìŠ¤(space) í‚¤ë¥¼ ëˆ„ë¦…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì•Œì•„ì„œ ìž¬ìƒë˜ê³ , ê°ì²´ ì¶”ì ì„ ì‹œìž‘í•©ë‹ˆë‹¤.   \n",
    "- í‚¤ë³´ë“œ 0~4 ìˆ«ìž í‚¤ë¥¼ ëˆŒëŸ¬ íŠ¸ëž™ì»¤ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.   \n",
    "- í™”ë©´ ìƒë‹¨ì˜ 0:TrackerBoosting, 1:TrackerMIL ë“±ì´ ëœ° ê²ë‹ˆë‹¤.   \n",
    "- í˜„ìž¬ ì–´ë–¤ íŠ¸ëž™ì»¤ë¡œ ê°ì²´ ì¶”ì ì„ í•˜ê³  ìžˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œì‹œìž…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracker APIs (track_trackingAPI.py)\n",
    "\n",
    "import cv2\n",
    "\n",
    "trackers = [cv2.TrackerBoosting_create,  # trackers idx no = 0\n",
    "            cv2.TrackerMIL_create,       # trackers idx no = 1\n",
    "            cv2.TrackerKCF_create,       # trackers idx no = 2\n",
    "            cv2.TrackerTLD_create,       # trackers idx no = 3\n",
    "            cv2.TrackerMedianFlow_create,# trackers idx no = 4\n",
    "#             cv2.TrackerGOTURN_create, # not work\n",
    "#             cv2.TrackerCSRT_create,   # not work\n",
    "#             cv2.TrackerMOSSE_create   # not work\n",
    "           ]\n",
    "trackerIdx = 0     # trackers idx no\n",
    "tracker = None\n",
    "isFirst = True\n",
    "\n",
    "video_src_f = './Videos/car_drive.mp4'\n",
    "cap = cv2.VideoCapture(video_src_f)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "delay = int(1000/fps)\n",
    "win_name = 'Tracking APIs'\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Cannot read video file')\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    if tracker is None:            \n",
    "        cv2.putText(img_draw, \"Press the Space to set ROI!!\", \\\n",
    "            (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "    else:\n",
    "        ok, bbox = tracker.update(frame)   # set roi\n",
    "        (x,y,w,h) = bbox\n",
    "        if ok:                             # track\n",
    "            cv2.rectangle(img_draw, (int(x), int(y)), (int(x + w), int(y + h)), (0,255,0), 2, 1)\n",
    "        else : \n",
    "            cv2.putText(img_draw, \"Tracking fail.\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "    trackerName = tracker.__class__.__name__\n",
    "    cv2.putText(img_draw, str(trackerIdx) + \":\"+trackerName , (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0),2,cv2.LINE_AA)\n",
    "    cv2.imshow(win_name, img_draw)\n",
    "    key = cv2.waitKey(delay) & 0xff\n",
    "   \n",
    "    if key == ord(' ') or (video_src_f != 0 and isFirst):  # space bar\n",
    "        isFirst = False\n",
    "        roi = cv2.selectROI(win_name, frame, False)        # set roi\n",
    "        if roi[2] and roi[3]:        \n",
    "            tracker = trackers[trackerIdx]()    \n",
    "            isInit = tracker.init(frame, roi)\n",
    "    elif key in range(48, 56):                             # if tracker idx has changeed input 0~4\n",
    "        trackerIdx = key-48     \n",
    "        if bbox is not None:\n",
    "            tracker = trackers[trackerIdx]() \n",
    "            isInit = tracker.init(frame, bbox) \n",
    "    elif key == 27 : \n",
    "        break\n",
    "# else:\n",
    "#     print( \"Could not open video\")\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–£ Single Object Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,! What tracker API would you like to use? \n",
      "Enter 0 for BOOSTING\n",
      "Enter 1 for MIL\n",
      "Enter 2 for KCF\n",
      "Enter 3 for TLD\n",
      "Enter 4 for MEDIANFLOW\n",
      "Enter 5 for GOTURN\n",
      "Enter 6 for MOSSE\n",
      "Enter 7 for CSRT\n",
      "Enter your tracker 4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def ask_for_tracker():\n",
    "    print('Hi,! What tracker API would you like to use? ')\n",
    "    print('Enter 0 for BOOSTING')\n",
    "    print('Enter 1 for MIL')\n",
    "    print('Enter 2 for KCF')\n",
    "    print('Enter 3 for TLD')\n",
    "    print('Enter 4 for MEDIANFLOW')\n",
    "    print('Enter 5 for GOTURN')\n",
    "    print('Enter 6 for MOSSE')\n",
    "    print('Enter 7 for CSRT')\n",
    "    \n",
    "    choise = input('Enter your tracker ')\n",
    "    \n",
    "    if choise == '0':\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    elif choise == '1':\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    elif choise == '2':\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    elif choise == '3':\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    elif choise == '4':\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "    elif choise == '5':\n",
    "        tracker = cv2.TrackerGOTURN_create()\n",
    "    elif choise == '6':\n",
    "        tracker = cv2.TrackerMOSSE_create()\n",
    "    elif choise == '7':\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "        \n",
    "    return tracker\n",
    "\n",
    "tracker = ask_for_tracker()            # Tracker\n",
    "tracker_name = str(tracker).split()[0][1:]  # Tracker name\n",
    "cap = cv2.VideoCapture('./Videos/Vehicles.mp4')\n",
    "\n",
    "ret, frame = cap.read()\n",
    "roi = cv2.selectROI(frame, False)\n",
    "ret = tracker.init(frame, roi)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    success, roi = tracker.update(frame)  # update tracker\n",
    "    (x, y, w, h) = tuple(map(int, roi))   # roi -> from tuple to int\n",
    "    cv2.putText(frame, str(tracker), (100,120), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0),2,cv2.LINE_AA)\n",
    "    \n",
    "    if success:                           # Sucess on tracking\n",
    "        pts1 = (x,y)\n",
    "        pts2 = (x+w, y+h)\n",
    "        cv2.rectangle(frame, pts1, pts2, (255, 125, 25), 3)  # Draw rects as tracker moves\n",
    "        \n",
    "    else:                      # Failure on tracking  \n",
    "        cv2.putText(frame, 'Fail to trace the object',(100, 100),cv2.FONT_HERSHEY_SIMPLEX, 1, (25, 125, 255),3)\n",
    "    \n",
    "    cv2.putText(frame,tracker_name,(20, 400),cv2.FONT_HERSHEY_SIMPLEX, 1, (225, 225, 0), 3)  # Display Tracker\n",
    "    cv2.imshow(tracker_name, frame)\n",
    "\n",
    "    if cv2.waitKey(50) & 0xFF == 27:\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–£ Multi Object Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tracking algorithm 'KCF' \n",
      "Available algorithms are: \n",
      "\n",
      "BOOSTING\n",
      "MIL\n",
      "KCF\n",
      "TLD\n",
      "MEADIANFLOW\n",
      "GOTURN\n",
      "MOSSE\n",
      "CSRT\n",
      "Press q to stop selecting boxes and start multitracking\n",
      "Press any key to select another box\n",
      "Selected boxes [(787, 389, 79, 75)]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "from random import randint\n",
    "tracker_types = ['BOOSTING',\n",
    "                'MIL',\n",
    "                'KCF',\n",
    "                'TLD',\n",
    "                'MEADIANFLOW',\n",
    "                'GOTURN',\n",
    "                'MOSSE',\n",
    "                'CSRT']\n",
    "\n",
    "def tracker_name(tracker_type):\n",
    "\n",
    "    if tracker_type == tracker_types[0]:\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    elif tracker_type == tracker_types[1]:\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    elif tracker_type == tracker_types[2]:\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    elif tracker_type == tracker_types[3]:\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    elif tracker_type == tracker_types[4]:\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "    elif tracker_type == tracker_types[5]:\n",
    "        tracker = cv2.TrackerGOTURN_create()\n",
    "    elif tracker_type == tracker_types[6]:\n",
    "        tracker = cv2.TrackerMOSSE_create()\n",
    "    elif tracker_type == tracker_types[7]:\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "    else:\n",
    "        tracker = None\n",
    "        print('No tracker found')\n",
    "        print('Choose from this trackers: ')\n",
    "        for tr in tracker_types:\n",
    "            print(tr)\n",
    "                                    \n",
    "    return tracker        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Default tracking algorithm 'KCF' \\n\" \"Available algorithms are: \\n\")\n",
    "    for ta in tracker_types:\n",
    "        print(ta)\n",
    "    tracker_type = 'KCF'\n",
    "    cap = cv2.VideoCapture('./Videos/Vehicles.mp4')\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print('Cannot read the video')\n",
    "    rects = []\n",
    "    colors = []\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        rect_box = cv2.selectROI('Multitracker', frame)\n",
    "        rects.append(rect_box)\n",
    "        colors.append((randint(64, 255), randint(64,255), randint(64,255)))\n",
    "        print('Press q to stop selecting boxes and start multitracking')\n",
    "        print('Press any key to select another box')\n",
    "\n",
    "        if cv2.waitKey(0) & 0xFF == 27:\n",
    "            break\n",
    "    print(f'Selected boxes {rects}')\n",
    "    multitracker = cv2.MultiTracker_create()  # Create multitracker\n",
    "    \n",
    "    for rect_box in rects:                    # Initialize multitracker\n",
    "        multitracker.add(tracker_name(tracker_type),frame,rect_box)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        success, boxes = multitracker.update(frame)  # update location objects\n",
    "        \n",
    "        for i, newbox in enumerate(boxes):    # draw the objectes tracked\n",
    "            pts1 = (int(newbox[0]), int(newbox[1]))\n",
    "            pts2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "            cv2.rectangle(frame, pts1, pts2, colors[i], 2, 1)\n",
    "        \n",
    "        cv2.imshow('Multitracker', frame)\n",
    "        \n",
    "        if cv2.waitKey(20) & 0xFF == 27:\n",
    "            break    \n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV-py36",
   "language": "python",
   "name": "cv_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
